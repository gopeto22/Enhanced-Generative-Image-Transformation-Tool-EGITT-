{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 184,
      "metadata": {
        "id": "rYR8ccgXbJby"
      },
      "outputs": [],
      "source": [
        "import importlib\n",
        "import torch.utils.data\n",
        "\n",
        "\n",
        "def find_dataset_using_name(dataset_name):\n",
        "    # Given the option --dataset [datasetname],\n",
        "    # the file \"datasets/datasetname_dataset.py\"\n",
        "    # will be imported.\n",
        "    dataset_filename = \"data.\" + dataset_name + \"_dataset\"\n",
        "    datasetlib = importlib.import_module(dataset_filename)\n",
        "\n",
        "    # In the file, the class called DatasetNameDataset() will\n",
        "    # be instantiated. It has to be a subclass of BaseDataset,\n",
        "    # and it is case-insensitive.\n",
        "    dataset = None\n",
        "    target_dataset_name = dataset_name.replace('_', '') + 'dataset'\n",
        "    for name, cls in datasetlib.__dict__.items():\n",
        "        if name.lower() == target_dataset_name.lower() \\\n",
        "           and issubclass(cls, BaseDataset):\n",
        "            dataset = cls\n",
        "\n",
        "    if dataset is None:\n",
        "        raise ValueError(\"In %s.py, there should be a subclass of BaseDataset \"\n",
        "                         \"with class name that matches %s in lowercase.\" %\n",
        "                         (dataset_filename, target_dataset_name))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def get_option_setter(dataset_name):\n",
        "    dataset_class = find_dataset_using_name(dataset_name)\n",
        "    return dataset_class.modify_commandline_options\n",
        "\n",
        "\n",
        "def create_dataloader(opt):\n",
        "    dataset = find_dataset_using_name(opt.dataset_mode)\n",
        "    instance = dataset()\n",
        "    instance.initialize(opt)\n",
        "    print(\"dataset [%s] of size %d was created\" %\n",
        "          (type(instance).__name__, len(instance)))\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        instance,\n",
        "        batch_size=opt.batchSize,\n",
        "        shuffle=not opt.serial_batches,\n",
        "        num_workers=int(opt.nThreads),\n",
        "        drop_last=opt.isTrain\n",
        "    )\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "class BaseDataset(data.Dataset):\n",
        "    def __init__(self):\n",
        "        super(BaseDataset, self).__init__()\n",
        "\n",
        "    @staticmethod\n",
        "    def modify_commandline_options(parser, is_train):\n",
        "        return parser\n",
        "\n",
        "    def initialize(self, opt):\n",
        "        pass\n",
        "\n",
        "\n",
        "def get_params(opt, size):\n",
        "    w, h = size\n",
        "    new_h = h\n",
        "    new_w = w\n",
        "    if opt.preprocess_mode == 'resize_and_crop':\n",
        "        new_h = new_w = opt.load_size\n",
        "    elif opt.preprocess_mode == 'scale_width_and_crop':\n",
        "        new_w = opt.load_size\n",
        "        new_h = opt.load_size * h // w\n",
        "    elif opt.preprocess_mode == 'scale_shortside_and_crop':\n",
        "        ss, ls = min(w, h), max(w, h)  # shortside and longside\n",
        "        width_is_shorter = w == ss\n",
        "        ls = int(opt.load_size * ls / ss)\n",
        "        new_w, new_h = (ss, ls) if width_is_shorter else (ls, ss)\n",
        "\n",
        "    x = random.randint(0, np.maximum(0, new_w - opt.crop_size))\n",
        "    y = random.randint(0, np.maximum(0, new_h - opt.crop_size))\n",
        "\n",
        "    flip = random.random() > 0.5\n",
        "    return {'crop_pos': (x, y), 'flip': flip}\n",
        "\n",
        "\n",
        "def get_transform(opt, params, method=Image.BICUBIC, normalize=True, toTensor=True):\n",
        "    transform_list = []\n",
        "    if 'resize' in opt.preprocess_mode:\n",
        "        osize = [opt.load_size, opt.load_size]\n",
        "        transform_list.append(transforms.Resize(osize, interpolation=method))\n",
        "    elif 'scale_width' in opt.preprocess_mode:\n",
        "        transform_list.append(transforms.Lambda(lambda img: __scale_width(img, opt.load_size, method)))\n",
        "    elif 'scale_shortside' in opt.preprocess_mode:\n",
        "        transform_list.append(transforms.Lambda(lambda img: __scale_shortside(img, opt.load_size, method)))\n",
        "\n",
        "    if 'crop' in opt.preprocess_mode:\n",
        "        transform_list.append(transforms.Lambda(lambda img: __crop(img, params['crop_pos'], opt.crop_size)))\n",
        "\n",
        "    if opt.preprocess_mode == 'none':\n",
        "        base = 32\n",
        "        transform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base, method)))\n",
        "\n",
        "    if opt.preprocess_mode == 'fixed':\n",
        "        w = opt.crop_size\n",
        "        h = round(opt.crop_size / opt.aspect_ratio)\n",
        "        transform_list.append(transforms.Lambda(lambda img: __resize(img, w, h, method)))\n",
        "\n",
        "    if opt.isTrain and not opt.no_flip:\n",
        "        transform_list.append(transforms.Lambda(lambda img: __flip(img, params['flip'])))\n",
        "\n",
        "    if toTensor:\n",
        "        transform_list += [transforms.ToTensor()]\n",
        "\n",
        "    if normalize:\n",
        "        transform_list += [transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                                                (0.5, 0.5, 0.5))]\n",
        "    return transforms.Compose(transform_list)\n",
        "\n",
        "\n",
        "def normalize():\n",
        "    return transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\n",
        "\n",
        "def __resize(img, w, h, method=Image.BICUBIC):\n",
        "    return img.resize((w, h), method)\n",
        "\n",
        "\n",
        "def __make_power_2(img, base, method=Image.BICUBIC):\n",
        "    ow, oh = img.size\n",
        "    h = int(round(oh / base) * base)\n",
        "    w = int(round(ow / base) * base)\n",
        "    if (h == oh) and (w == ow):\n",
        "        return img\n",
        "    return img.resize((w, h), method)\n",
        "\n",
        "\n",
        "def __scale_width(img, target_width, method=Image.BICUBIC):\n",
        "    ow, oh = img.size\n",
        "    if (ow == target_width):\n",
        "        return img\n",
        "    w = target_width\n",
        "    h = int(target_width * oh / ow)\n",
        "    return img.resize((w, h), method)\n",
        "\n",
        "\n",
        "def __scale_shortside(img, target_width, method=Image.BICUBIC):\n",
        "    ow, oh = img.size\n",
        "    ss, ls = min(ow, oh), max(ow, oh)  # shortside and longside\n",
        "    width_is_shorter = ow == ss\n",
        "    if (ss == target_width):\n",
        "        return img\n",
        "    ls = int(target_width * ls / ss)\n",
        "    nw, nh = (ss, ls) if width_is_shorter else (ls, ss)\n",
        "    return img.resize((nw, nh), method)\n",
        "\n",
        "\n",
        "def __crop(img, pos, size):\n",
        "    ow, oh = img.size\n",
        "    x1, y1 = pos\n",
        "    tw = th = size\n",
        "    return img.crop((x1, y1, x1 + tw, y1 + th))\n",
        "\n",
        "\n",
        "def __flip(img, flip):\n",
        "    if flip:\n",
        "        return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    return img"
      ],
      "metadata": {
        "id": "Ry1Nq8JdbPnE"
      },
      "execution_count": 185,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "class BaseDataset(data.Dataset):\n",
        "    def __init__(self):\n",
        "        super(BaseDataset, self).__init__()\n",
        "\n",
        "    @staticmethod\n",
        "    def modify_commandline_options(parser, is_train):\n",
        "        return parser\n",
        "\n",
        "    def initialize(self, opt):\n",
        "        pass\n",
        "\n",
        "\n",
        "def get_params(opt, size):\n",
        "    w, h = size\n",
        "    new_h = h\n",
        "    new_w = w\n",
        "    if opt.preprocess_mode == 'resize_and_crop':\n",
        "        new_h = new_w = opt.load_size\n",
        "    elif opt.preprocess_mode == 'scale_width_and_crop':\n",
        "        new_w = opt.load_size\n",
        "        new_h = opt.load_size * h // w\n",
        "    elif opt.preprocess_mode == 'scale_shortside_and_crop':\n",
        "        ss, ls = min(w, h), max(w, h)  # shortside and longside\n",
        "        width_is_shorter = w == ss\n",
        "        ls = int(opt.load_size * ls / ss)\n",
        "        new_w, new_h = (ss, ls) if width_is_shorter else (ls, ss)\n",
        "\n",
        "    x = random.randint(0, np.maximum(0, new_w - opt.crop_size))\n",
        "    y = random.randint(0, np.maximum(0, new_h - opt.crop_size))\n",
        "\n",
        "    flip = random.random() > 0.5\n",
        "    return {'crop_pos': (x, y), 'flip': flip}\n",
        "\n",
        "\n",
        "def get_transform(opt, params, method=Image.BICUBIC, normalize=True, toTensor=True):\n",
        "    transform_list = []\n",
        "    if 'resize' in opt.preprocess_mode:\n",
        "        osize = [opt.load_size, opt.load_size]\n",
        "        transform_list.append(transforms.Resize(osize, interpolation=method))\n",
        "    elif 'scale_width' in opt.preprocess_mode:\n",
        "        transform_list.append(transforms.Lambda(lambda img: __scale_width(img, opt.load_size, method)))\n",
        "    elif 'scale_shortside' in opt.preprocess_mode:\n",
        "        transform_list.append(transforms.Lambda(lambda img: __scale_shortside(img, opt.load_size, method)))\n",
        "\n",
        "    if 'crop' in opt.preprocess_mode:\n",
        "        transform_list.append(transforms.Lambda(lambda img: __crop(img, params['crop_pos'], opt.crop_size)))\n",
        "\n",
        "    if opt.preprocess_mode == 'none':\n",
        "        base = 32\n",
        "        transform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base, method)))\n",
        "\n",
        "    if opt.preprocess_mode == 'fixed':\n",
        "        w = opt.crop_size\n",
        "        h = round(opt.crop_size / opt.aspect_ratio)\n",
        "        transform_list.append(transforms.Lambda(lambda img: __resize(img, w, h, method)))\n",
        "\n",
        "    if opt.isTrain and not opt.no_flip:\n",
        "        transform_list.append(transforms.Lambda(lambda img: __flip(img, params['flip'])))\n",
        "\n",
        "    if toTensor:\n",
        "        transform_list += [transforms.ToTensor()]\n",
        "\n",
        "    if normalize:\n",
        "        transform_list += [transforms.Normalize((0.5, 0.5, 0.5),\n",
        "                                                (0.5, 0.5, 0.5))]\n",
        "    return transforms.Compose(transform_list)\n",
        "\n",
        "\n",
        "def normalize():\n",
        "    return transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\n",
        "\n",
        "def __resize(img, w, h, method=Image.BICUBIC):\n",
        "    return img.resize((w, h), method)\n",
        "\n",
        "\n",
        "def __make_power_2(img, base, method=Image.BICUBIC):\n",
        "    ow, oh = img.size\n",
        "    h = int(round(oh / base) * base)\n",
        "    w = int(round(ow / base) * base)\n",
        "    if (h == oh) and (w == ow):\n",
        "        return img\n",
        "    return img.resize((w, h), method)\n",
        "\n",
        "\n",
        "def __scale_width(img, target_width, method=Image.BICUBIC):\n",
        "    ow, oh = img.size\n",
        "    if (ow == target_width):\n",
        "        return img\n",
        "    w = target_width\n",
        "    h = int(target_width * oh / ow)\n",
        "    return img.resize((w, h), method)\n",
        "\n",
        "\n",
        "def __scale_shortside(img, target_width, method=Image.BICUBIC):\n",
        "    ow, oh = img.size\n",
        "    ss, ls = min(ow, oh), max(ow, oh)  # shortside and longside\n",
        "    width_is_shorter = ow == ss\n",
        "    if (ss == target_width):\n",
        "        return img\n",
        "    ls = int(target_width * ls / ss)\n",
        "    nw, nh = (ss, ls) if width_is_shorter else (ls, ss)\n",
        "    return img.resize((nw, nh), method)\n",
        "\n",
        "\n",
        "def __crop(img, pos, size):\n",
        "    ow, oh = img.size\n",
        "    x1, y1 = pos\n",
        "    tw = th = size\n",
        "    return img.crop((x1, y1, x1 + tw, y1 + th))\n",
        "\n",
        "\n",
        "def __flip(img, flip):\n",
        "    if flip:\n",
        "        return img.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    return img"
      ],
      "metadata": {
        "id": "PH_7bcNfc0cW"
      },
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "\n",
        "class Pix2pixDataset(BaseDataset):\n",
        "    @staticmethod\n",
        "    def modify_commandline_options(parser, is_train):\n",
        "        parser.add_argument('--no_pairing_check', action='store_true',\n",
        "                            help='If specified, skip sanity check of correct label-image file pairing')\n",
        "        return parser\n",
        "\n",
        "    def initialize(self, opt):\n",
        "        self.opt = opt\n",
        "\n",
        "        label_paths, image_paths, instance_paths = self.get_paths(opt)\n",
        "\n",
        "        util.natural_sort(label_paths)\n",
        "        util.natural_sort(image_paths)\n",
        "        if not opt.no_instance:\n",
        "            util.natural_sort(instance_paths)\n",
        "\n",
        "        label_paths = label_paths[:opt.max_dataset_size]\n",
        "        image_paths = image_paths[:opt.max_dataset_size]\n",
        "        instance_paths = instance_paths[:opt.max_dataset_size]\n",
        "\n",
        "        if not opt.no_pairing_check:\n",
        "            for path1, path2 in zip(label_paths, image_paths):\n",
        "                assert self.paths_match(path1, path2), \\\n",
        "                    \"The label-image pair (%s, %s) do not look like the right pair because the filenames are quite different. Are you sure about the pairing? Please see data/pix2pix_dataset.py to see what is going on, and use --no_pairing_check to bypass this.\" % (path1, path2)\n",
        "\n",
        "        self.label_paths = label_paths\n",
        "        self.image_paths = image_paths\n",
        "        self.instance_paths = instance_paths\n",
        "\n",
        "        size = len(self.label_paths)\n",
        "        self.dataset_size = size\n",
        "\n",
        "    def get_paths(self, opt):\n",
        "        label_paths = []\n",
        "        image_paths = []\n",
        "        instance_paths = []\n",
        "        assert False, \"A subclass of Pix2pixDataset must override self.get_paths(self, opt)\"\n",
        "        return label_paths, image_paths, instance_paths\n",
        "\n",
        "    def paths_match(self, path1, path2):\n",
        "        filename1_without_ext = os.path.splitext(os.path.basename(path1))[0]\n",
        "        filename2_without_ext = os.path.splitext(os.path.basename(path2))[0]\n",
        "        return filename1_without_ext == filename2_without_ext\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Label (Content) Image\n",
        "        label_path = self.label_paths[index]\n",
        "        label = Image.open(label_path)\n",
        "        if self.opt.task != 'SIS':\n",
        "            label = label.convert('RGB')\n",
        "        params = get_params(self.opt, label.size)\n",
        "\n",
        "        if self.opt.task != 'SIS':\n",
        "            transform_label = get_transform(self.opt, params)\n",
        "            label_tensor = transform_label(label)\n",
        "        else:\n",
        "            transform_label = get_transform(self.opt, params, method=Image.NEAREST, normalize=False)\n",
        "            label_tensor = transform_label(label) * 255.0\n",
        "            label_tensor[label_tensor == 255] = self.opt.label_nc  # 'unknown' is opt.label_nc\n",
        "\n",
        "        # Real (Style) Image\n",
        "        image_path = self.image_paths[index]\n",
        "        assert self.paths_match(label_path, image_path), \\\n",
        "            \"The label_path %s and image_path %s don't match.\" % \\\n",
        "            (label_path, image_path)\n",
        "        image = Image.open(image_path)\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "        transform_image = get_transform(self.opt, params)\n",
        "        image_tensor = transform_image(image)\n",
        "\n",
        "        # if using instance maps\n",
        "        if self.opt.no_instance:\n",
        "            instance_tensor = 0\n",
        "        else:\n",
        "            instance_path = self.instance_paths[index]\n",
        "            instance = Image.open(instance_path)\n",
        "            if instance.mode == 'L':\n",
        "                instance_tensor = transform_label(instance) * 255\n",
        "                instance_tensor = instance_tensor.long()\n",
        "            else:\n",
        "                instance_tensor = transform_label(instance)\n",
        "\n",
        "        input_dict = {'label': label_tensor,\n",
        "                      'instance': instance_tensor,\n",
        "                      'image': image_tensor,\n",
        "                      'path': image_path,\n",
        "                      'cpath': label_path\n",
        "                      }\n",
        "\n",
        "        # Give subclasses a chance to modify the final output\n",
        "        self.postprocess(input_dict)\n",
        "\n",
        "        return input_dict\n",
        "\n",
        "    def postprocess(self, input_dict):\n",
        "        return input_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size"
      ],
      "metadata": {
        "id": "nlrasb70cx2T"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "class CityscapesDataset(Pix2pixDataset):\n",
        "\n",
        "    @staticmethod\n",
        "    def modify_commandline_options(parser, is_train):\n",
        "        parser = Pix2pixDataset.modify_commandline_options(parser, is_train)\n",
        "        parser.set_defaults(preprocess_mode='fixed')\n",
        "        parser.set_defaults(load_size=512)\n",
        "        parser.set_defaults(crop_size=512)\n",
        "        parser.set_defaults(display_winsize=512)\n",
        "        parser.set_defaults(label_nc=35)\n",
        "        parser.set_defaults(aspect_ratio=2.0)\n",
        "        parser.set_defaults(batchSize=16)\n",
        "        opt, _ = parser.parse_known_args()\n",
        "        if hasattr(opt, 'num_upsampling_layers'):\n",
        "            parser.set_defaults(num_upsampling_layers='more')\n",
        "        return parser\n",
        "\n",
        "    def get_paths(self, opt):\n",
        "        root = opt.croot\n",
        "        phase = 'val' if opt.phase == 'test' else 'train'\n",
        "\n",
        "        label_dir = os.path.join(root, 'gtFine', phase)\n",
        "        label_paths_all = make_dataset(label_dir, recursive=True)\n",
        "        label_paths = [p for p in label_paths_all if p.endswith('_labelIds.png')]\n",
        "\n",
        "        image_dir = os.path.join(root, 'leftImg8bit', phase)\n",
        "        image_paths = make_dataset(image_dir, recursive=True)\n",
        "\n",
        "        if not opt.no_instance:\n",
        "            instance_paths = [p for p in label_paths_all if p.endswith('_instanceIds.png')]\n",
        "        else:\n",
        "            instance_paths = []\n",
        "\n",
        "        return label_paths, image_paths, instance_paths\n",
        "\n",
        "    def paths_match(self, path1, path2):\n",
        "        name1 = os.path.basename(path1)\n",
        "        name2 = os.path.basename(path2)\n",
        "        # compare the first 3 components, [city]_[id1]_[id2]\n",
        "        return '_'.join(name1.split('_')[:3]) == \\\n",
        "            '_'.join(name2.split('_')[:3])"
      ],
      "metadata": {
        "id": "ZZvG_wiEbQtr"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################\n",
        "# Code from\n",
        "# https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py\n",
        "# Modified the original code so that it also loads images from the current\n",
        "# directory as well as the subdirectories\n",
        "###############################################################################\n",
        "import torch.utils.data as data\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "IMG_EXTENSIONS = [\n",
        "    '.jpg', '.JPG', '.jpeg', '.JPEG',\n",
        "    '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP', '.tiff', '.webp'\n",
        "]\n",
        "\n",
        "\n",
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n",
        "\n",
        "\n",
        "def make_dataset_rec(dir, images):\n",
        "#    assert os.path.isdir(dir), '%s is not a valid directory' % dir\n",
        "\n",
        "    for root, dnames, fnames in sorted(os.walk(dir, followlinks=True)):\n",
        "        for fname in fnames:\n",
        "            if is_image_file(fname):\n",
        "                path = os.path.join(root, fname)\n",
        "                images.append(path)\n",
        "\n",
        "\n",
        "def make_dataset(dir, recursive=False, read_cache=False, write_cache=False):\n",
        "    images = []\n",
        "\n",
        "    if read_cache:\n",
        "        possible_filelist = os.path.join(dir, 'files.list')\n",
        "        if os.path.isfile(possible_filelist):\n",
        "            with open(possible_filelist, 'r') as f:\n",
        "                images = f.read().splitlines()\n",
        "                return images\n",
        "\n",
        "  #  if recursive:\n",
        "#        make_dataset_rec(dir, images)\n",
        "    else:\n",
        "#        assert os.path.isdir(dir) or os.path.islink(dir), '%s is not a valid directory' % dir\n",
        "\n",
        "        for root, dnames, fnames in sorted(os.walk(dir)):\n",
        "            for fname in fnames:\n",
        "                if is_image_file(fname):\n",
        "                    path = os.path.join(root, fname)\n",
        "                    images.append(path)\n",
        "\n",
        "    if write_cache:\n",
        "        filelist_cache = os.path.join(dir, 'files.list')\n",
        "        with open(filelist_cache, 'w') as f:\n",
        "            for path in images:\n",
        "                f.write(\"%s\\n\" % path)\n",
        "            print('wrote filelist cache at %s' % filelist_cache)\n",
        "\n",
        "    return images\n",
        "\n",
        "\n",
        "def default_loader(path):\n",
        "    return Image.open(path).convert('RGB')\n",
        "\n",
        "\n",
        "class ImageFolder(data.Dataset):\n",
        "\n",
        "    def __init__(self, root, transform=None, return_paths=False,\n",
        "                 loader=default_loader):\n",
        "        imgs = make_dataset(root)\n",
        "        if len(imgs) == 0:\n",
        "            raise(RuntimeError(\"Found 0 images in: \" + root + \"\\n\"\n",
        "                               \"Supported image extensions are: \" +\n",
        "                               \",\".join(IMG_EXTENSIONS)))\n",
        "\n",
        "        self.root = root\n",
        "        self.imgs = imgs\n",
        "        self.transform = transform\n",
        "        self.return_paths = return_paths\n",
        "        self.loader = loader\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        path = self.imgs[index]\n",
        "        img = self.loader(path)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        if self.return_paths:\n",
        "            return img, path\n",
        "        else:\n",
        "            return img\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "metadata": {
        "id": "53LZsGrMbSQU"
      },
      "execution_count": 189,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Pix2pixDataset(BaseDataset):\n",
        "    @staticmethod\n",
        "    def modify_commandline_options(parser, is_train):\n",
        "        parser.add_argument('--no_pairing_check', action='store_true',\n",
        "                            help='If specified, skip sanity check of correct label-image file pairing')\n",
        "        return parser\n",
        "\n",
        "    def initialize(self, opt):\n",
        "        self.opt = opt\n",
        "\n",
        "        label_paths, image_paths, instance_paths = self.get_paths(opt)\n",
        "\n",
        "        util.natural_sort(label_paths)\n",
        "        util.natural_sort(image_paths)\n",
        "        if not opt.no_instance:\n",
        "            util.natural_sort(instance_paths)\n",
        "\n",
        "        label_paths = label_paths[:opt.max_dataset_size]\n",
        "        image_paths = image_paths[:opt.max_dataset_size]\n",
        "        instance_paths = instance_paths[:opt.max_dataset_size]\n",
        "\n",
        "        if not opt.no_pairing_check:\n",
        "            for path1, path2 in zip(label_paths, image_paths):\n",
        "                assert self.paths_match(path1, path2), \\\n",
        "                    \"The label-image pair (%s, %s) do not look like the right pair because the filenames are quite different. Are you sure about the pairing? Please see data/pix2pix_dataset.py to see what is going on, and use --no_pairing_check to bypass this.\" % (path1, path2)\n",
        "\n",
        "        self.label_paths = label_paths\n",
        "        self.image_paths = image_paths\n",
        "        self.instance_paths = instance_paths\n",
        "\n",
        "        size = len(self.label_paths)\n",
        "        self.dataset_size = size\n",
        "\n",
        "    def get_paths(self, opt):\n",
        "        label_paths = []\n",
        "        image_paths = []\n",
        "        instance_paths = []\n",
        "        assert False, \"A subclass of Pix2pixDataset must override self.get_paths(self, opt)\"\n",
        "        return label_paths, image_paths, instance_paths\n",
        "\n",
        "    def paths_match(self, path1, path2):\n",
        "        filename1_without_ext = os.path.splitext(os.path.basename(path1))[0]\n",
        "        filename2_without_ext = os.path.splitext(os.path.basename(path2))[0]\n",
        "        return filename1_without_ext == filename2_without_ext\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Label (Content) Image\n",
        "        label_path = self.label_paths[index]\n",
        "        label = Image.open(label_path)\n",
        "        if self.opt.task != 'SIS':\n",
        "            label = label.convert('RGB')\n",
        "        params = get_params(self.opt, label.size)\n",
        "\n",
        "        if self.opt.task != 'SIS':\n",
        "            transform_label = get_transform(self.opt, params)\n",
        "            label_tensor = transform_label(label)\n",
        "        else:\n",
        "            transform_label = get_transform(self.opt, params, method=Image.NEAREST, normalize=False)\n",
        "            label_tensor = transform_label(label) * 255.0\n",
        "            label_tensor[label_tensor == 255] = self.opt.label_nc  # 'unknown' is opt.label_nc\n",
        "\n",
        "        # Real (Style) Image\n",
        "        image_path = self.image_paths[index]\n",
        "        assert self.paths_match(label_path, image_path), \\\n",
        "            \"The label_path %s and image_path %s don't match.\" % \\\n",
        "            (label_path, image_path)\n",
        "        image = Image.open(image_path)\n",
        "        image = image.convert('RGB')\n",
        "\n",
        "        transform_image = get_transform(self.opt, params)\n",
        "        image_tensor = transform_image(image)\n",
        "\n",
        "        # if using instance maps\n",
        "        if self.opt.no_instance:\n",
        "            instance_tensor = 0\n",
        "        else:\n",
        "            instance_path = self.instance_paths[index]\n",
        "            instance = Image.open(instance_path)\n",
        "            if instance.mode == 'L':\n",
        "                instance_tensor = transform_label(instance) * 255\n",
        "                instance_tensor = instance_tensor.long()\n",
        "            else:\n",
        "                instance_tensor = transform_label(instance)\n",
        "\n",
        "        input_dict = {'label': label_tensor,\n",
        "                      'instance': instance_tensor,\n",
        "                      'image': image_tensor,\n",
        "                      'path': image_path,\n",
        "                      'cpath': label_path\n",
        "                      }\n",
        "\n",
        "        # Give subclasses a chance to modify the final output\n",
        "        self.postprocess(input_dict)\n",
        "\n",
        "        return input_dict\n",
        "\n",
        "    def postprocess(self, input_dict):\n",
        "        return input_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size"
      ],
      "metadata": {
        "id": "NdqXEIekbThQ"
      },
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "class Summer2WinterYosemiteDataset(Pix2pixDataset):\n",
        "\n",
        "    @staticmethod\n",
        "    def modify_commandline_options(parser, is_train):\n",
        "        parser = Pix2pixDataset.modify_commandline_options(parser, is_train)\n",
        "        parser.set_defaults(preprocess_mode='fixed')\n",
        "        parser.set_defaults(load_size=256)\n",
        "        parser.set_defaults(crop_size=256)\n",
        "        parser.set_defaults(display_winsize=256)\n",
        "        parser.set_defaults(aspect_ratio=1.0)\n",
        "        opt, _ = parser.parse_known_args()\n",
        "        if hasattr(opt, 'num_upsampling_layers'):\n",
        "            parser.set_defaults(num_upsampling_layers='more')\n",
        "        return parser\n",
        "\n",
        "    def get_paths(self, opt):\n",
        "        croot = opt.croot\n",
        "        sroot = opt.sroot\n",
        "\n",
        "        c_image_dir = os.path.join(croot, '%sA' % opt.phase)\n",
        "        c_image_paths = sorted(make_dataset(c_image_dir, recursive=True))\n",
        "\n",
        "        s_image_dir = os.path.join(sroot, '%sB' % opt.phase)\n",
        "        s_image_paths = sorted(make_dataset(s_image_dir, recursive=True))\n",
        "\n",
        "        if opt.phase == 'train':\n",
        "            s_image_paths = s_image_paths + s_image_paths\n",
        "\n",
        "        instance_paths = []\n",
        "\n",
        "        length = min(len(c_image_paths), len(s_image_paths))\n",
        "        c_image_paths = c_image_paths[:length]\n",
        "        s_image_paths = s_image_paths[:length]\n",
        "        return c_image_paths, s_image_paths, instance_paths\n",
        "\n",
        "    def paths_match(self, path1, path2):\n",
        "        return True"
      ],
      "metadata": {
        "id": "W9ixtiujbUvw"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# This function calculates the mean and standard deviation of a feature tensor. It supports the option of calculating\n",
        "# these statistics only within regions specified by a mask.\n",
        "def calc_mean_std(feat, eps=1e-5, mask=None):\n",
        "    size = feat.size()\n",
        "    N, C = size[:2]\n",
        "\n",
        "    # If mask is provided, calculate mean and std within masked regions\n",
        "    if mask is not None:\n",
        "        cnt = mask.view(N, 1, -1).sum(2)\n",
        "        mf = mask * feat\n",
        "        mf_flat = mf.view(N, C, -1)\n",
        "        mf_sum = mf_flat.sum(2)\n",
        "\n",
        "        feat_mean = mf_sum / cnt\n",
        "        feat_var = (mf_flat ** 2).sum(2) / cnt - feat_mean ** 2\n",
        "        feat_std = feat_var.sqrt() + eps\n",
        "        feat_std = feat_std.view(N, C, 1, 1)\n",
        "        feat_mean = feat_mean.view(N, C, 1, 1)\n",
        "    else:\n",
        "        # Calculate mean and std over the entire feature tensor\n",
        "        feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
        "        feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
        "        feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
        "\n",
        "    return feat_mean, feat_std\n",
        "\n",
        "# This function performs adaptive instance normalization between two feature tensors, content_feat and style_feat,\n",
        "# considering optional masks for both content and style.\n",
        "def adaptive_instance_normalization(content_feat, style_feat, c_mask=None, s_mask=None):\n",
        "    assert content_feat.size()[:2] == style_feat.size()[:2]\n",
        "\n",
        "    size = content_feat.size()\n",
        "    H, W = size[2], size[3]\n",
        "\n",
        "    # Interpolate masks to match the spatial dimensions of the feature tensors\n",
        "    msk = F.interpolate(c_mask, (H, W)) if c_mask is not None else None\n",
        "    s_msk = F.interpolate(s_mask, (H, W)) if s_mask is not None else None\n",
        "\n",
        "    # Calculate mean and std for both content and style\n",
        "    style_mean, style_std = calc_mean_std(style_feat, mask=s_msk)\n",
        "    content_mean, content_std = calc_mean_std(content_feat, mask=msk)\n",
        "\n",
        "    # Normalize content feature\n",
        "    normalized_feat = (content_feat - content_mean.expand(size)) / content_std.expand(size)\n",
        "\n",
        "    if msk is not None:\n",
        "        # Apply normalized style to content within masked regions\n",
        "        return (normalized_feat * style_std.expand(size) + style_mean.expand(size)) * msk + content_feat * (1 - msk)\n",
        "    else:\n",
        "        # Apply normalized style to entire content\n",
        "        return normalized_feat * style_std.expand(size) + style_mean.expand(size)\n",
        "\n",
        "# This function calculates the mean and standard deviation of a flattened 3D feature tensor within its channels.\n",
        "def _calc_feat_flatten_mean_std(feat):\n",
        "    assert (feat.size()[0] == 3)\n",
        "    assert (isinstance(feat, torch.FloatTensor))\n",
        "    feat_flatten = feat.view(3, -1)\n",
        "    mean = feat_flatten.mean(dim=-1, keepdim=True)\n",
        "    std = feat_flatten.std(dim=-1, keepdim=True)\n",
        "    return feat_flatten, mean, std\n",
        "\n",
        "# This function computes the square root of a matrix using Singular Value Decomposition (SVD).\n",
        "def _mat_sqrt(x):\n",
        "    U, D, V = torch.svd(x)\n",
        "    return torch.mm(torch.mm(U, D.pow(0.5).diag()), V.t())\n",
        "\n",
        "# This function performs CORAL (Covariate Shift Reduction) between two 3D feature tensors (source and target).\n",
        "def coral(source, target):\n",
        "    # Flatten and normalize source and target features\n",
        "    source_f, source_f_mean, source_f_std = _calc_feat_flatten_mean_std(source)\n",
        "    source_f_norm = (source_f - source_f_mean.expand_as(source_f)) / source_f_std.expand_as(source_f)\n",
        "    source_f_cov_eye = torch.mm(source_f_norm, source_f_norm.t()) + torch.eye(3)\n",
        "\n",
        "    target_f, target_f_mean, target_f_std = _calc_feat_flatten_mean_std(target)\n",
        "    target_f_norm = (target_f - target_f_mean.expand_as(target_f)) / target_f_std.expand_as(target_f)\n",
        "    target_f_cov_eye = torch.mm(target_f_norm, target_f_norm.t()) + torch.eye(3)\n",
        "\n",
        "    # Transform source feature to reduce covariate shift\n",
        "    source_f_norm_transfer = torch.mm(\n",
        "        _mat_sqrt(target_f_cov_eye),\n",
        "        torch.mm(torch.inverse(_mat_sqrt(source_f_cov_eye)),\n",
        "                 source_f_norm)\n",
        "    )\n",
        "\n",
        "    # Rescale and shift source feature to match target statistics\n",
        "    source_f_transfer = source_f_norm_transfer * target_f_std.expand_as(source_f_norm) + target_f_mean.expand_as(source_f_norm)\n",
        "\n",
        "    return source_f_transfer.view(source.size())\n"
      ],
      "metadata": {
        "id": "bMOBUVaWbauS"
      },
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def find_network_using_name(target_network_name, filename):\n",
        "    target_class_name = target_network_name + filename\n",
        "    module_name = filename\n",
        "    network = find_class_in_module(target_class_name, module_name)\n",
        "\n",
        "    assert issubclass(network, BaseNetwork), \\\n",
        "        \"Class %s should be a subclass of BaseNetwork\" % network\n",
        "\n",
        "    return network\n",
        "\n",
        "\n",
        "def modify_commandline_options(parser, is_train):\n",
        "    opt, _ = parser.parse_known_args()\n",
        "\n",
        "    netG_cls = find_network_using_name(opt.netG, 'generator')\n",
        "    parser = netG_cls.modify_commandline_options(parser, is_train)\n",
        "    if is_train:\n",
        "        netD_cls = find_network_using_name(opt.netD, 'discriminator')\n",
        "        parser = netD_cls.modify_commandline_options(parser, is_train)\n",
        "    netE_cls = find_network_using_name('conv', 'encoder')\n",
        "    parser = netE_cls.modify_commandline_options(parser, is_train)\n",
        "\n",
        "    return parser\n",
        "\n",
        "\n",
        "def create_network(cls, opt):\n",
        "    net = cls(opt)\n",
        "    net.print_network()\n",
        "\n",
        "    if len(opt.gpu_ids) > 0:\n",
        "        if torch.cuda.is_available():\n",
        "            net.cuda()\n",
        "        else:\n",
        "            print(\"Warning: CUDA is not available. Running on CPU.\")\n",
        "\n",
        "    net.init_weights(opt.init_type, opt.init_variance)\n",
        "    return net\n",
        "\n",
        "\n",
        "\n",
        "def define_G(opt):\n",
        "    netG_cls = find_network_using_name(opt.netG, 'generator')\n",
        "    return create_network(netG_cls, opt)\n",
        "\n",
        "\n",
        "def define_D(opt):\n",
        "    netD_cls = find_network_using_name(opt.netD, 'discriminator')\n",
        "    return create_network(netD_cls, opt)\n",
        "\n",
        "\n",
        "def define_E(opt):\n",
        "    # there exists only one encoder type\n",
        "    netE_cls = find_network_using_name('conv', 'encoder')\n",
        "    return create_network(netE_cls, opt)"
      ],
      "metadata": {
        "id": "N9Xse5ojbinR"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.nn.utils.spectral_norm as spectral_norm\n",
        "\n",
        "\n",
        "# ResNet block that uses FADE.\n",
        "# It differs from the ResNet block of SPADE in that\n",
        "# it takes in the feature map as input, learns the skip connection if necessary.\n",
        "# This architecture seemed like a standard architecture for unconditional or\n",
        "# class-conditional GAN architecture using residual block.\n",
        "# The code was inspired from https://github.com/LMescheder/GAN_stability\n",
        "# and https://github.com/NVlabs/SPADE.\n",
        "class FADEResnetBlock(nn.Module):\n",
        "    def __init__(self, fin, fout, opt):\n",
        "        super().__init__()\n",
        "        # attributes\n",
        "        self.learned_shortcut = (fin != fout)\n",
        "        fmiddle = fin\n",
        "\n",
        "        # create conv layers\n",
        "        self.conv_0 = nn.Conv2d(fin, fmiddle, kernel_size=3, padding=1)\n",
        "        self.conv_1 = nn.Conv2d(fmiddle, fout, kernel_size=3, padding=1)\n",
        "        if self.learned_shortcut:\n",
        "            self.conv_s = nn.Conv2d(fin, fout, kernel_size=1, bias=False)\n",
        "\n",
        "        # apply spectral norm if specified\n",
        "        if 'spectral' in opt.norm_G:\n",
        "            self.conv_0 = spectral_norm(self.conv_0)\n",
        "            self.conv_1 = spectral_norm(self.conv_1)\n",
        "            if self.learned_shortcut:\n",
        "                self.conv_s = spectral_norm(self.conv_s)\n",
        "\n",
        "        # define normalization layers\n",
        "        fade_config_str = opt.norm_G.replace('spectral', '')\n",
        "        self.norm_0 = FADE(fade_config_str, fin, fin)\n",
        "        self.norm_1 = FADE(fade_config_str, fmiddle, fmiddle)\n",
        "        if self.learned_shortcut:\n",
        "            self.norm_s = FADE(fade_config_str, fin, fin)\n",
        "\n",
        "    # Note the resnet block with FADE also takes in |feat|,\n",
        "    # the feature map as input\n",
        "    def forward(self, x, feat):\n",
        "        x_s = self.shortcut(x, feat)\n",
        "\n",
        "        dx = self.conv_0(self.actvn(self.norm_0(x, feat)))\n",
        "        dx = self.conv_1(self.actvn(self.norm_1(dx, feat)))\n",
        "\n",
        "        out = x_s + dx\n",
        "\n",
        "        return out\n",
        "\n",
        "    def shortcut(self, x, feat):\n",
        "        if self.learned_shortcut:\n",
        "            x_s = self.conv_s(self.norm_s(x, feat))\n",
        "        else:\n",
        "            x_s = x\n",
        "        return x_s\n",
        "\n",
        "    def actvn(self, x):\n",
        "        return F.leaky_relu(x, 2e-1)\n",
        "\n",
        "\n",
        "class StreamResnetBlock(nn.Module):\n",
        "    def __init__(self, fin, fout, opt):\n",
        "        super().__init__()\n",
        "        # attributes\n",
        "        self.learned_shortcut = (fin != fout)\n",
        "        fmiddle = fin\n",
        "\n",
        "        # create conv layers\n",
        "        self.conv_0 = nn.Conv2d(fin, fmiddle, kernel_size=3, padding=1)\n",
        "        self.conv_1 = nn.Conv2d(fmiddle, fout, kernel_size=3, padding=1)\n",
        "        if self.learned_shortcut:\n",
        "            self.conv_s = nn.Conv2d(fin, fout, kernel_size=1, bias=False)\n",
        "\n",
        "        # apply spectral norm if specified\n",
        "        if 'spectral' in opt.norm_S:\n",
        "            self.conv_0 = spectral_norm(self.conv_0)\n",
        "            self.conv_1 = spectral_norm(self.conv_1)\n",
        "            if self.learned_shortcut:\n",
        "                self.conv_s = spectral_norm(self.conv_s)\n",
        "\n",
        "        # define normalization layers\n",
        "        subnorm_type = opt.norm_S.replace('spectral', '')\n",
        "        if subnorm_type == 'batch':\n",
        "            self.norm_layer_in = nn.BatchNorm2d(fin, affine=True)\n",
        "            self.norm_layer_out= nn.BatchNorm2d(fout, affine=True)\n",
        "            if self.learned_shortcut:\n",
        "                self.norm_layer_s = nn.BatchNorm2d(fout, affine=True)\n",
        "        elif subnorm_type == 'syncbatch':\n",
        "            self.norm_layer_in = SynchronizedBatchNorm2d(fin, affine=True)\n",
        "            self.norm_layer_out= SynchronizedBatchNorm2d(fout, affine=True)\n",
        "            if self.learned_shortcut:\n",
        "                self.norm_layer_s = SynchronizedBatchNorm2d(fout, affine=True)\n",
        "        elif subnorm_type == 'instance':\n",
        "            self.norm_layer_in = nn.InstanceNorm2d(fin, affine=False)\n",
        "            self.norm_layer_out= nn.InstanceNorm2d(fout, affine=False)\n",
        "            if self.learned_shortcut:\n",
        "                self.norm_layer_s = nn.InstanceNorm2d(fout, affine=False)\n",
        "        else:\n",
        "            raise ValueError('normalization layer %s is not recognized' % subnorm_type)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_s = self.shortcut(x)\n",
        "\n",
        "        dx = self.actvn(self.norm_layer_in(self.conv_0(x)))\n",
        "        dx = self.actvn(self.norm_layer_out(self.conv_1(dx)))\n",
        "\n",
        "        out = x_s + dx\n",
        "\n",
        "        return out\n",
        "\n",
        "    def shortcut(self,x):\n",
        "        if self.learned_shortcut:\n",
        "            x_s = self.actvn(self.norm_layer_s(self.conv_s(x)))\n",
        "        else:\n",
        "            x_s = x\n",
        "        return x_s\n",
        "\n",
        "    def actvn(self, x):\n",
        "        return F.leaky_relu(x, 2e-1)\n",
        "\n",
        "\n",
        "# ResNet block used in pix2pixHD\n",
        "# We keep the same architecture as pix2pixHD.\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, norm_layer, activation=nn.ReLU(False), kernel_size=3):\n",
        "        super().__init__()\n",
        "\n",
        "        pw = (kernel_size - 1) // 2\n",
        "        self.conv_block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(pw),\n",
        "            norm_layer(nn.Conv2d(dim, dim, kernel_size=kernel_size)),\n",
        "            activation,\n",
        "            nn.ReflectionPad2d(pw),\n",
        "            norm_layer(nn.Conv2d(dim, dim, kernel_size=kernel_size))\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.conv_block(x)\n",
        "        out = x + y\n",
        "        return out\n",
        "\n",
        "\n",
        "# VGG architecture, used for the perceptual loss using a pretrained VGG network\n",
        "class VGG19(torch.nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super().__init__()\n",
        "        vgg_pretrained_features = torchvision.vgg19(pretrained=True).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        self.slice5 = torch.nn.Sequential()\n",
        "        for x in range(2):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(2, 7):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(7, 12):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(12, 21):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(21, 30):\n",
        "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def forward(self, X):\n",
        "        h_relu1 = self.slice1(X)\n",
        "        h_relu2 = self.slice2(h_relu1)\n",
        "        h_relu3 = self.slice3(h_relu2)\n",
        "        h_relu4 = self.slice4(h_relu3)\n",
        "        h_relu5 = self.slice5(h_relu4)\n",
        "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
        "        return out"
      ],
      "metadata": {
        "id": "QUB4FCsAbpJO"
      },
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "\n",
        "class BaseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BaseNetwork, self).__init__()\n",
        "\n",
        "    # Method for modifying command line options if required\n",
        "    # Here = no-op and simply returns the original parser\n",
        "    @staticmethod\n",
        "    def modify_commandline_options(parser, is_train):\n",
        "        #This method could be overriden in subclasses to add/ modify command line options\n",
        "        return parser\n",
        "\n",
        "    # Method for prinitng information about the network architecture\n",
        "    def print_network(self):\n",
        "        # Calculates and prints the total number of parameters in the network\n",
        "        num_params = sum(param.numel() for param in self.parameters())\n",
        "        print(f\"Network [{self.__class__.__name__}] was created. \"\n",
        "              f\"Total numbers of parameters: {num_params/ 1e6:.1f} million. \"\n",
        "              \"To see the architecture, use print(network).\")\n",
        "\n",
        "    # This method initialises weights of the network based on the specified initialisation method\n",
        "    # Supports various initialisation methods - normal, xavier, kaiming, orthogonal or none\n",
        "    # For convolutional and linear layers, apply the specified initialisation method to weights and biases\n",
        "    def init_weights(self, init_type = 'normal', gain = 0.02):\n",
        "        def init_func(m):\n",
        "            classname = m.__class__.__name__\n",
        "\n",
        "            # Initialise 2D batch norm layers\n",
        "            if 'BatchNorm2d' in classname:\n",
        "                init.normal_(getattr(m, 'weight', None), 1.0, gain)\n",
        "\n",
        "            # Initialise Conv2d and Linear Layers\n",
        "            elif hasattr(m, 'weight') and (\"Conv\" in classname or \"Linear\" in classname):\n",
        "                weight_attr = getattr(m, 'weight', None)\n",
        "                bias_attr = getattr(m, 'bias', None)\n",
        "\n",
        "                if init_type == 'normal':\n",
        "                    init.normal_(weight_attr, 0.0, gain)\n",
        "                elif init_type == 'xavier':\n",
        "                    init.xavier_normal_(weight_attr, gain = gain)\n",
        "                elif init_type == 'xavier_uniform':\n",
        "                    init.xavier_uniform_(weight_attr, gain = 1.0)\n",
        "                elif init_type == \"kaiming\":\n",
        "                    init.kaiming_normal_(weight_attr, a = 0, mode = 'fan_in')\n",
        "                elif init_type == 'orthogonal':\n",
        "                    init.orthogonal_(weight_attr, gain = gain)\n",
        "                elif init_type == 'none':\n",
        "                    # Use PyTorch's default initialisation method\n",
        "                    m.reset_parameters()\n",
        "                else:\n",
        "                    raise NotImplementedError(f'Initialisation method [{init_type}] is not implemented.')\n",
        "\n",
        "                # Initialise bias if present\n",
        "                if bias_attr is not None:\n",
        "                    init.constant_(bias_attr, 0.0)\n",
        "\n",
        "        # Apply initialisation function to network's parameters\n",
        "        self.apply(init_func)\n",
        "\n",
        "        # Propagate the initialisation to children (recursive initialisaton)\n",
        "        for m in self.children():\n",
        "            if hasattr(m, 'init_weights'):\n",
        "                m.init_weights(init_type, gain)\n"
      ],
      "metadata": {
        "id": "3PtnmpRjbq4h"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# File   : comm.py\n",
        "# Author : Jiayuan Mao\n",
        "# Email  : maojiayuan@gmail.com\n",
        "# Date   : 27/01/2018\n",
        "#\n",
        "# This file is part of Synchronized-BatchNorm-PyTorch.\n",
        "# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n",
        "# Distributed under MIT License.\n",
        "\n",
        "import queue\n",
        "import collections\n",
        "import threading\n",
        "\n",
        "__all__ = ['FutureResult', 'SlavePipe', 'SyncMaster']\n",
        "\n",
        "\n",
        "class FutureResult(object):\n",
        "    \"\"\"A thread-safe future implementation. Used only as one-to-one pipe.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self._result = None\n",
        "        self._lock = threading.Lock()\n",
        "        self._cond = threading.Condition(self._lock)\n",
        "\n",
        "    def put(self, result):\n",
        "        with self._lock:\n",
        "            assert self._result is None, 'Previous result has\\'t been fetched.'\n",
        "            self._result = result\n",
        "            self._cond.notify()\n",
        "\n",
        "    def get(self):\n",
        "        with self._lock:\n",
        "            if self._result is None:\n",
        "                self._cond.wait()\n",
        "\n",
        "            res = self._result\n",
        "            self._result = None\n",
        "            return res\n",
        "\n",
        "\n",
        "_MasterRegistry = collections.namedtuple('MasterRegistry', ['result'])\n",
        "_SlavePipeBase = collections.namedtuple('_SlavePipeBase', ['identifier', 'queue', 'result'])\n",
        "\n",
        "\n",
        "class SlavePipe(_SlavePipeBase):\n",
        "    \"\"\"Pipe for master-slave communication.\"\"\"\n",
        "\n",
        "    def run_slave(self, msg):\n",
        "        self.queue.put((self.identifier, msg))\n",
        "        ret = self.result.get()\n",
        "        self.queue.put(True)\n",
        "        return ret\n",
        "\n",
        "\n",
        "class SyncMaster(object):\n",
        "    \"\"\"An abstract `SyncMaster` object.\n",
        "\n",
        "    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n",
        "    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n",
        "    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n",
        "    and passed to a registered callback.\n",
        "    - After receiving the messages, the master device should gather the information and determine to message passed\n",
        "    back to each slave devices.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, master_callback):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            master_callback: a callback to be invoked after having collected messages from slave devices.\n",
        "        \"\"\"\n",
        "        self._master_callback = master_callback\n",
        "        self._queue = queue.Queue()\n",
        "        self._registry = collections.OrderedDict()\n",
        "        self._activated = False\n",
        "\n",
        "    def __getstate__(self):\n",
        "        return {'master_callback': self._master_callback}\n",
        "\n",
        "    def __setstate__(self, state):\n",
        "        self.__init__(state['master_callback'])\n",
        "\n",
        "    def register_slave(self, identifier):\n",
        "        \"\"\"\n",
        "        Register an slave device.\n",
        "\n",
        "        Args:\n",
        "            identifier: an identifier, usually is the device id.\n",
        "\n",
        "        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n",
        "\n",
        "        \"\"\"\n",
        "        if self._activated:\n",
        "            assert self._queue.empty(), 'Queue is not clean before next initialization.'\n",
        "            self._activated = False\n",
        "            self._registry.clear()\n",
        "        future = FutureResult()\n",
        "        self._registry[identifier] = _MasterRegistry(future)\n",
        "        return SlavePipe(identifier, self._queue, future)\n",
        "\n",
        "    def run_master(self, master_msg):\n",
        "        \"\"\"\n",
        "        Main entry for the master device in each forward pass.\n",
        "        The messages were first collected from each devices (including the master device), and then\n",
        "        an callback will be invoked to compute the message to be sent back to each devices\n",
        "        (including the master device).\n",
        "\n",
        "        Args:\n",
        "            master_msg: the message that the master want to send to itself. This will be placed as the first\n",
        "            message when calling `master_callback`. For detailed usage, see `_SynchronizedBatchNorm` for an example.\n",
        "\n",
        "        Returns: the message to be sent back to the master device.\n",
        "\n",
        "        \"\"\"\n",
        "        self._activated = True\n",
        "\n",
        "        intermediates = [(0, master_msg)]\n",
        "        for i in range(self.nr_slaves):\n",
        "            intermediates.append(self._queue.get())\n",
        "\n",
        "        results = self._master_callback(intermediates)\n",
        "        assert results[0][0] == 0, 'The first result should belongs to the master.'\n",
        "\n",
        "        for i, res in results:\n",
        "            if i == 0:\n",
        "                continue\n",
        "            self._registry[i].result.put(res)\n",
        "\n",
        "        for i in range(self.nr_slaves):\n",
        "            assert self._queue.get() is True\n",
        "\n",
        "        return results[0][1]\n",
        "\n",
        "    @property\n",
        "    def nr_slaves(self):\n",
        "        return len(self._registry)"
      ],
      "metadata": {
        "id": "S4CiLdXlbsWh"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class MultiscaleDiscriminator(BaseNetwork):\n",
        "    @staticmethod\n",
        "    def modify_commandline_options(parser, is_train):\n",
        "        # Adding a command line option\n",
        "        parser.add_argument('--netD_subarch',type = str, default = 'n_layer', help = 'architecture of each discriminator')\n",
        "        # One more\n",
        "        parser.add_argument('--num_D',type = int, default = 2, help = 'number of discriminators used in multiscale')\n",
        "        # Parsing the two command line arguments\n",
        "        opt, _ = parser.parse_known_args()\n",
        "\n",
        "        # Define properties of each discriminator of the multiscale discriminator\n",
        "        # Finding a class dynamically\n",
        "        subnetD = find_class_in_module(getattr(opt, 'netD_subarch') + 'discriminator', 'discriminator')\n",
        "        # Modifying command line options using the found class\n",
        "        subnetD.modify_commandline_options(parser, is_train)\n",
        "\n",
        "        return parser\n",
        "\n",
        "    def __init__(self,opt):\n",
        "        # Calling the constuctor of the base class\n",
        "        super().__init__()\n",
        "        # Storing possibilites\n",
        "        self.opt = opt\n",
        "        # List for holding multiple discriminators\n",
        "        self.discriminators = nn.ModuleList()\n",
        "\n",
        "        # Looping, based on number of discriminators\n",
        "        for _ in range(opt.num_D):\n",
        "            # Creating and adding discriminators to the list\n",
        "            self.discriminators.append(self.crete_single_discriminator(opt))\n",
        "\n",
        "    def createSingleDiscriminator(self, opt):\n",
        "        # Getting the discriminator subarchitecture option\n",
        "        subarch = opt.netD_subarch\n",
        "        # Mapping subarchitectures to corresponding classes\n",
        "        discriminator_options = {'n_layer': NLayerDiscriminator}\n",
        "\n",
        "        if subarch in discriminator_options:\n",
        "            # Create an instance of the specified discriminator class\n",
        "            return discriminator_options[subarch](opt)\n",
        "        else:\n",
        "            # Handle an unrecognised architecture\n",
        "            raise ValueError(f'unrecognised discriminator subarchitecture {subarch}')\n",
        "\n",
        "    def downsample(self,input):\n",
        "        # Perform downsampling using average pooling\n",
        "        return F.avg_pool2d(input, kernel_size = 3, stride = 2, padding = [1,1], count_include_pad = False)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Forward pass through each discriminator in the list\n",
        "        return [D(input) for D in self.discriminators]\n",
        "\n",
        "class NLayerDiscriminator(BaseNetwork):\n",
        "    @staticmethod\n",
        "    def modify_commandline_options(parser, is_train):\n",
        "        # Adiing a command line option\n",
        "        parser.add_argument('--n_layers_D', type = int, default = 4, help = '# layers at each discriminator')\n",
        "        return parser\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super().__init__()\n",
        "        if opt.task != 'SIS':\n",
        "            # Modifying number of layers based on a conditon\n",
        "            opt.n_layers_D = 3\n",
        "        # Storing options\n",
        "        self.opt = opt\n",
        "\n",
        "        # Set a kernel size\n",
        "        kw = 4\n",
        "        # Calculate padding based on kernel size\n",
        "        padw = int(np.ceil(kw - 1.0) / 2)\n",
        "        # Get number of filters\n",
        "        nf = opt.ndf\n",
        "        # Calculate input channels based on options\n",
        "        norm_layer = get_norm_layer(opt, opt.norm_D)\n",
        "        # Create a list to hold the layers in the discriminator\n",
        "        sequence = []\n",
        "\n",
        "        # Looping based on number of layers\n",
        "        for n in range(opt.n_layers_D):\n",
        "            # Storing previous number of filters\n",
        "            nf_prev = nf\n",
        "            # Doubling number of filters with a maximum of 512\n",
        "            nf = min(nf * 2, 512)\n",
        "            # Set stride based on layer index\n",
        "            stride = 1 if n == 3 else 2\n",
        "            # Add convolution layer\n",
        "            sequence += [norm_layer(nn.Conv2d(nf_prev, nf, kernel_size = kw, stride = stride, padding = padw)),\n",
        "            # Add a leaky ReLU\n",
        "            nn.LeakyReLU(0.2, False)]\n",
        "\n",
        "        # Adding a final convolutional layer\n",
        "        sequence += [nn.Conv2d(nf, 1, kernel_size = kw, stride = 1, padding = padw)]\n",
        "\n",
        "        # Divide layers into groups to extract intermediate layer outputs\n",
        "        # Looping through the layers\n",
        "        for n, layer in enumerate(sequence):\n",
        "            # Add layers as modules\n",
        "            self.add_module(f'model{n}', nn.Sequential(layer))\n",
        "\n",
        "    def compute_D_input_nc(self, opt):\n",
        "        # Getting output channel based on options\n",
        "        input_nc = opt.output_nc\n",
        "        if opt.task == 'SIS':\n",
        "            # Adding label channels if the task is semantic image synthesis\n",
        "            input_nc = opt.label_nc\n",
        "            if opt.contain_dontcare_label:\n",
        "                # Add additional channel for dontcare labels\n",
        "                input_nc += 1\n",
        "            if not opt.no_instance:\n",
        "                # Add additional channel for instance map\n",
        "                input_nc += 1\n",
        "        # Return computed input channel\n",
        "        return input_nc\n",
        "\n",
        "    def forward(self,input):\n",
        "        # Store input in results list\n",
        "        results = [input]\n",
        "        # Looping through the submodels / layers\n",
        "        for submodel in self.children:\n",
        "            # Forward pass through the curent layer\n",
        "            intermidiate_output = submodel(results[-1])\n",
        "            # Storing intermediate output in the results list\n",
        "            results.append(intermidiate_output)\n",
        "\n",
        "        # Check whether intermediate features are needed\n",
        "        get_intermediate_features = not self.opt.no_ganFeat_loss\n",
        "        # Return intermediate features/ final output\n",
        "        return results[1:] if get_intermediate_features else results[-1]"
      ],
      "metadata": {
        "id": "VWnO7APBbtiJ"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class convencoder(BaseNetwork):\n",
        "    \"\"\" Same architecture as the image discriminator \"\"\"\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super().__init__()  # Call the constructor of the parent class (BaseNetwork)\n",
        "        self.opt = opt  # Store the options for later use\n",
        "        kw = 3  # Kernel size for convolutional layers\n",
        "        pw = int(np.ceil((kw - 1.0) / 2))  # Padding width to maintain spatial dimensions\n",
        "        ndf = opt.ngf  # Number of filters in the first convolutional layer\n",
        "        norm_layer = get_norm_layer(opt, opt.norm_E)  # Get normalization layer based on options\n",
        "\n",
        "        # Define the first convolutional layer with normalization and activation\n",
        "        self.layer1 = norm_layer(nn.Conv2d(3, ndf, kw, stride=2, padding=pw, activation=self.actvn))\n",
        "\n",
        "        # Define a sequence of convolutional layers with increasing filter sizes\n",
        "        conv_layers = []\n",
        "        for i in range(1, 7 if self.opt.crop_size >= 256 else 6):\n",
        "            conv_layers.append(norm_layer(nn.Conv2d(ndf * (2**(i-1)), ndf * (2**i), kw, stride=2, padding=pw, activation=self.actvn)))\n",
        "        self.conv_layers = nn.Sequential(*conv_layers)  # Create a sequential container for convolutional layers\n",
        "\n",
        "        self.so = s0 = 4  # Dimensionality after spatial downsampling\n",
        "        self.fc_mu = nn.Linear(ndf * 8 * s0 * s0, 256)  # Fully connected layer for mean\n",
        "        self.fc_var = nn.Linear(ndf * 8 * s0 * s0, 256)  # Fully connected layer for log-variance\n",
        "\n",
        "        self.actvn = nn.LeakyReLU(0.2, False)  # Activation function (Leaky ReLU)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Resize input if needed using bilinear interpolation\n",
        "        if x.size(2) != 256 or x.size(3) != 256:\n",
        "            x = F.interpolate(x, size=(256, 256), mode='bilinear', align_corners=False)\n",
        "\n",
        "        # Apply the first convolutional layer\n",
        "        x = self.layer1(x)\n",
        "\n",
        "        # Apply the sequence of convolutional layers\n",
        "        x = self.conv_layers(x)\n",
        "\n",
        "        # Apply the activation function\n",
        "        x = self.actvn(x)\n",
        "\n",
        "        # Flatten the output and apply fully connected layers for mean and log-variance\n",
        "        x = x.view(x.size(0), -1)\n",
        "        mu = self.fc_mu(x)\n",
        "        logvar = self.fc_var(x)\n",
        "\n",
        "        return mu, logvar\n"
      ],
      "metadata": {
        "id": "WV2Lm8Bhbusr"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "class tsitgenerator(BaseNetwork):\n",
        "    @staticmethod\n",
        "    def modify_commandline_options(parser, is_train):\n",
        "        # Set default normalization for the generator and add an argument for the number of upsampling layers\n",
        "        parser.set_defaults(norm_G='spectralfadesyncbatch3x3')\n",
        "        parser.add_argument('--num_upsampling_layers',\n",
        "                            choices=('normal', 'more', 'most'), default='more',\n",
        "                            help=\"If 'more', adds upsampling layer between the two middle resnet blocks.\"\n",
        "                                 \"If 'most', also add one more upsampling + resnet layer at the end of the generator.\"\n",
        "                                 \"We only use 'more' as the default setting.\")\n",
        "        return parser\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super().__init__()\n",
        "        self.opt = opt\n",
        "        nf = opt.ngf\n",
        "\n",
        "        # Define content and style streams using nn.ModuleList\n",
        "        self.content_stream = nn.ModuleList([Stream(self.opt) for _ in range(2)])\n",
        "        self.style_stream = nn.ModuleList([Stream(self.opt) for _ in range(2)]) if not self.opt.no_ss else None\n",
        "        self.sw, self.sh = self.compute_latent_vector_size(opt)\n",
        "\n",
        "        # Define the initial layer based on whether VAE is used or not\n",
        "        if opt.use_vae:\n",
        "            self.fc = nn.Linear(opt.z_dim, 16 * nf * self.sw * self.sh)\n",
        "        else:\n",
        "            self.fc = nn.Conv2d(self.opt.semantic_nc, 16 * nf, 3, padding=1)\n",
        "\n",
        "        # Define the generator blocks\n",
        "        self.head_0 = FADEResnetBlock(16 * nf, 16 * nf, opt)\n",
        "        self.G_middle_0 = FADEResnetBlock(16 * nf, 16 * nf, opt)\n",
        "        self.G_middle_1 = FADEResnetBlock(16 * nf, 16 * nf, opt)\n",
        "        self.up_0 = FADEResnetBlock(16 * nf, 8 * nf, opt)\n",
        "        self.up_1 = FADEResnetBlock(8 * nf, 4 * nf, opt)\n",
        "        self.up_2 = FADEResnetBlock(4 * nf, 2 * nf, opt)\n",
        "        self.up_3 = FADEResnetBlock(2 * nf, 1 * nf, opt)\n",
        "\n",
        "        final_nc = nf\n",
        "\n",
        "        # Additional upsampling block if specified\n",
        "        if opt.num_upsampling_layers == 'most':\n",
        "            self.up_4 = FADEResnetBlock(1 * nf, nf // 2, opt)\n",
        "            final_nc = nf // 2\n",
        "\n",
        "        # Output convolution layer\n",
        "        self.conv_img = nn.Conv2d(final_nc, 3, 3, padding=1)\n",
        "\n",
        "        # Upsampling layer\n",
        "        self.up = nn.Upsample(scale_factor=2)\n",
        "\n",
        "    def compute_latent_vector_size(self, opt):\n",
        "        # Compute the size of the latent vector based on the number of upsampling layers\n",
        "        if opt.num_upsampling_layers == 'normal':\n",
        "            num_up_layers = 6\n",
        "        elif opt.num_upsampling_layers == 'more':\n",
        "            num_up_layers = 7\n",
        "        elif opt.num_upsampling_layers == 'most':\n",
        "            num_up_layers = 8\n",
        "        else:\n",
        "            raise ValueError('opt.num_upsampling_layers [%s] not recognized' %\n",
        "                             opt.num_upsampling_layers)\n",
        "\n",
        "        sw = opt.crop_size // (2**num_up_layers)\n",
        "        sh = round(sw / opt.aspect_ratio)\n",
        "\n",
        "        return sw, sh\n",
        "\n",
        "    def fadain_alpha(self, content_feat, style_feat, alpha=1.0, c_mask=None, s_mask=None):\n",
        "        # FAdaIN performs AdaIN on the multi-scale feature representations\n",
        "        assert 0 <= alpha <= 1\n",
        "        t = F.AdaIn(content_feat, style_feat, c_mask, s_mask)\n",
        "        t = alpha * t + (1 - alpha) * content_feat\n",
        "        return t\n",
        "\n",
        "    def forward(self, input, real, z=None):\n",
        "        # Extract content and style features from streams\n",
        "        content = input\n",
        "        style = real\n",
        "        ft_list = [getattr(self.content_stream, f\"ft{i}\") for i in range(8)]\n",
        "        sft_list = [getattr(self.style_stream, f\"sft{i}\") if not self.opt.no_ss else None for i in range(8)]\n",
        "\n",
        "        # Generate the input tensor based on VAE or deterministic content\n",
        "        if self.opt.use_vae:\n",
        "            if z is None:\n",
        "                z = torch.randn(content.size(0), self.opt.z_dim,\n",
        "                                dtype=torch.float32, device=content.get_device())\n",
        "            x = self.fc(z)\n",
        "            x = x.view(-1, 16 * self.opt.ngf, self.sh, self.sw)\n",
        "        else:\n",
        "            if self.opt.task == 'SIS':\n",
        "                x = F.interpolate(content, size=(self.sh, self.sw))\n",
        "            else:\n",
        "                x = torch.randn(content.size(0), 3, self.sh, self.sw, dtype=torch.float32, device=content.get_device())\n",
        "            x = self.fc(x)\n",
        "\n",
        "        # Apply upsampling blocks with FAdaIN\n",
        "        for i in range(7):\n",
        "            x = self.up(x)\n",
        "            x = self.fadain_alpha(x, getattr(sft_list, f\"sft{i}\"), alpha=self.opt.alpha) if not self.opt.no_ss else x\n",
        "            x = getattr(self, f\"G_middle_{i}\")(x, getattr(ft_list, f\"ft{i}\"))\n",
        "\n",
        "        # Additional upsampling if specified\n",
        "        if self.opt.num_upsampling_layers == 'more' or self.opt.num_upsampling_layers == 'most':\n",
        "            x = self.up(x)\n",
        "\n",
        "        # Output convolution and activation\n",
        "        x = self.conv_img(F.leaky_relu(x, 2e-1))\n",
        "        x = F.tanh(x)\n",
        "        return x\n",
        "\n",
        "class Pix2PixHDGenerator(BaseNetwork):\n",
        "    @staticmethod\n",
        "    def modify_commandline_options(parser, is_train):\n",
        "        parser.add_argument('--resnet_n_downsample', type=int, default=4, help='number of downsampling layers in netG')\n",
        "        parser.add_argument('--resnet_n_blocks', type=int, default=9, help='number of residual blocks in the global generator network')\n",
        "        parser.add_argument('--resnet_kernel_size', type=int, default=3,\n",
        "                            help='kernel size of the resnet block')\n",
        "        parser.add_argument('--resnet_initial_kernel_size', type=int, default=7,\n",
        "                            help='kernel size of the first convolution')\n",
        "        parser.set_defaults(norm_G='instance')\n",
        "        return parser\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super().__init__()\n",
        "        input_nc = opt.label_nc + (1 if opt.contain_dontcare_label else 0) + (0 if opt.no_instance else 1)\n",
        "\n",
        "        norm_layer = get_norm_layer(opt, opt.norm_G)\n",
        "        activation = nn.ReLU(False)\n",
        "\n",
        "        model = []\n",
        "\n",
        "        # initial conv\n",
        "        model += [nn.ReflectionPad2d(opt.resnet_initial_kernel_size // 2),\n",
        "                  norm_layer(nn.Conv2d(input_nc, opt.ngf,\n",
        "                                       kernel_size=opt.resnet_initial_kernel_size,\n",
        "                                       padding=0)),\n",
        "                  activation]\n",
        "        # downsample\n",
        "        mult = 1\n",
        "        for i in range(opt.resnet_n_downsample):\n",
        "            model += [norm_layer(nn.Conv2d(opt.ngf * mult, opt.ngf * mult * 2,\n",
        "                                           kernel_size=3, stride=2, padding=1)),\n",
        "                      activation]\n",
        "            mult *= 2\n",
        "\n",
        "        # resnet blocks\n",
        "        for i in range(opt.resnet_n_blocks):\n",
        "            model += [ResnetBlock(opt.ngf * mult,\n",
        "                                  norm_layer=norm_layer,\n",
        "                                  activation=activation,\n",
        "                                  kernel_size=opt.resnet_kernel_size)]\n",
        "\n",
        "        # upsample\n",
        "        for i in range(opt.resnet_n_downsample):\n",
        "            nc_in = int(opt.ngf * mult)\n",
        "            nc_out = int((opt.ngf * mult) / 2)\n",
        "            model += [norm_layer(nn.ConvTranspose2d(nc_in, nc_out,\n",
        "                                                    kernel_size=3, stride=2,\n",
        "                                                    padding=1, output_padding=1)),\n",
        "                      activation]\n",
        "            mult = mult // 2\n",
        "        # final output conv\n",
        "        model += [nn.ReflectionPad2d(3),\n",
        "                  nn.Conv2d(nc_out, opt.output_nc, kernel_size=7, padding=0),\n",
        "                  nn.Tanh()]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, input, z=None):\n",
        "        return self.model(input)\n"
      ],
      "metadata": {
        "id": "hoZD9Vlbbv5Y"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class GANLoss(nn.Module):\n",
        "    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0,\n",
        "                 tensor=torch.FloatTensor, opt=None):\n",
        "        super(GANLoss, self).__init__()\n",
        "\n",
        "        # Set up initial parameters\n",
        "        self.real_label = target_real_label\n",
        "        self.fake_label = target_fake_label\n",
        "        self.Tensor = tensor\n",
        "        self.gan_mode = gan_mode\n",
        "        self.opt = opt\n",
        "        self.real_label_tensor, self.fake_label_tensor, self.zero_tensor = None, None, None\n",
        "\n",
        "        # Common initialization based on gan_mode\n",
        "        if gan_mode == 'ls':\n",
        "            pass\n",
        "        elif gan_mode in ['original', 'w', 'hinge']:\n",
        "            pass\n",
        "        else:\n",
        "            raise ValueError('Unexpected gan_mode {}'.format(gan_mode))\n",
        "\n",
        "    def get_target_tensor(self, input, target_is_real):\n",
        "        # Get the target tensor based on whether it should be real or fake\n",
        "        target_tensor = (self.real_label if target_is_real else self.fake_label)\n",
        "        if target_is_real:\n",
        "            return self.real_label_tensor.expand_as(input)\n",
        "        else:\n",
        "            return self.fake_label_tensor.expand_as(input)\n",
        "\n",
        "    def get_zero_tensor(self, input):\n",
        "        # Get a tensor of zeros with the same shape as the input\n",
        "        return self.zero_tensor.expand_as(input)\n",
        "\n",
        "    def loss(self, input, target_is_real, for_discriminator=True):\n",
        "        # Calculate GAN loss based on the specified gan_mode\n",
        "\n",
        "        target_tensor = self.get_target_tensor(input, target_is_real)\n",
        "        zero_tensor = self.get_zero_tensor(input)\n",
        "\n",
        "        if self.gan_mode == 'original':\n",
        "            # Binary cross-entropy loss\n",
        "            return F.binary_cross_entropy_with_logits(input, target_tensor)\n",
        "        elif self.gan_mode == 'ls':\n",
        "            # Least squares loss\n",
        "            return F.mse_loss(input, target_tensor)\n",
        "        elif self.gan_mode == 'hinge':\n",
        "            # Hinge loss\n",
        "            minval = torch.min(input - 1, zero_tensor) if target_is_real else torch.min(-input - 1, zero_tensor)\n",
        "            loss = -torch.mean(minval) if for_discriminator else -torch.mean(input)\n",
        "            return loss\n",
        "        else:\n",
        "            # Wasserstein GAN (wgan)\n",
        "            return -input.mean() if target_is_real else input.mean()\n",
        "\n",
        "    def __call__(self, input, target_is_real, for_discriminator=True):\n",
        "        # Compute GAN loss, handling the case when input is a list of tensors\n",
        "\n",
        "        if isinstance(input, list):\n",
        "            # If input is a list (multiscale discriminator), compute loss for each scale\n",
        "            loss = 0\n",
        "            for pred_i in input:\n",
        "                if isinstance(pred_i, list):\n",
        "                    pred_i = pred_i[-1]\n",
        "                loss_tensor = self.loss(pred_i, target_is_real, for_discriminator)\n",
        "                bs = 1 if len(loss_tensor.size()) == 0 else loss_tensor.size(0)\n",
        "                new_loss = torch.mean(loss_tensor.view(bs, -1), dim=1)\n",
        "                loss += new_loss\n",
        "            return loss / len(input)\n",
        "        else:\n",
        "            # If input is a single tensor, compute GAN loss directly\n",
        "            return self.loss(input, target_is_real, for_discriminator)\n",
        "\n",
        "class VGGLoss(nn.Module):\n",
        "    def __init__(self, gpu_ids):\n",
        "        super(VGGLoss, self).__init__()\n",
        "\n",
        "        # Set up VGG model and L1 loss\n",
        "        self.vgg = VGG19().cuda()\n",
        "        self.criterion = nn.L1Loss()\n",
        "        self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0]\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Forward pass for VGG loss, calculating perceptual loss\n",
        "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
        "        loss = sum(w * self.criterion(x_, y_.detach()) for w, x_, y_ in zip(self.weights, x_vgg, y_vgg))\n",
        "        return loss\n",
        "\n",
        "class KLDLoss(nn.Module):\n",
        "    def forward(self, mu, logvar):\n",
        "        # Forward pass for KL Divergence loss used in VAE\n",
        "        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n"
      ],
      "metadata": {
        "id": "in1mPvJQbxTG"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.nn.utils.spectral_norm as spectral_norm\n",
        "\n",
        "# Enums or Constants for normalization types\n",
        "SPECTRAL_NORM = 'spectral'\n",
        "INSTANCE_NORM = 'instance'\n",
        "SYNC_BATCH_NORM = 'syncbatch'\n",
        "BATCH_NORM = 'batch'\n",
        "NONE_NORM = 'none'\n",
        "\n",
        "# Returns a function that creates a standard normalization function\n",
        "def get_norm_layer(opt, norm_type='instance'):\n",
        "    def get_out_channel(layer):\n",
        "        return getattr(layer, 'out_channels', layer.weight.size(0))\n",
        "\n",
        "    def add_norm_layer(layer):\n",
        "        nonlocal norm_type\n",
        "        if norm_type.startswith(SPECTRAL_NORM):\n",
        "            layer = spectral_norm(layer)\n",
        "            subnorm_type = norm_type[len(SPECTRAL_NORM):]\n",
        "        else:\n",
        "            subnorm_type = norm_type\n",
        "\n",
        "        if subnorm_type == NONE_NORM:\n",
        "            return layer\n",
        "\n",
        "        if getattr(layer, 'bias', None) is not None:\n",
        "            delattr(layer, 'bias')\n",
        "            layer.register_parameter('bias', None)\n",
        "\n",
        "        norm_layer = None\n",
        "        if subnorm_type == BATCH_NORM:\n",
        "            norm_layer = nn.BatchNorm2d(get_out_channel(layer), affine=True)\n",
        "        elif subnorm_type == SYNC_BATCH_NORM:\n",
        "            norm_layer = SynchronizedBatchNorm2d(get_out_channel(layer), affine=True)\n",
        "        elif subnorm_type == INSTANCE_NORM:\n",
        "            norm_layer = nn.InstanceNorm2d(get_out_channel(layer), affine=False)\n",
        "        else:\n",
        "            raise ValueError('Normalization layer %s is not recognized' % subnorm_type)\n",
        "\n",
        "        return nn.Sequential(layer, norm_layer)\n",
        "\n",
        "    return add_norm_layer\n",
        "\n",
        "# Creates FADE normalization layer based on the given configuration\n",
        "class FADE(nn.Module):\n",
        "    def __init__(self, config_text, norm_nc, label_nc):\n",
        "        super().__init__()\n",
        "\n",
        "        assert config_text.startswith('fade')\n",
        "        parsed = re.search('fade(\\D+)(\\d)x\\d', config_text)\n",
        "        param_free_norm_type = str(parsed.group(1))\n",
        "        ks = int(parsed.group(2))\n",
        "\n",
        "        if param_free_norm_type == INSTANCE_NORM:\n",
        "            self.param_free_norm = nn.InstanceNorm2d(norm_nc, affine=False)\n",
        "        elif param_free_norm_type == SYNC_BATCH_NORM:\n",
        "            self.param_free_norm = SynchronizedBatchNorm2d(norm_nc, affine=False)\n",
        "        elif param_free_norm_type == BATCH_NORM:\n",
        "            self.param_free_norm = nn.BatchNorm2d(norm_nc, affine=False)\n",
        "        else:\n",
        "            raise ValueError('%s is not a recognized param-free norm type in FADE'\n",
        "                             % param_free_norm_type)\n",
        "\n",
        "        pw = ks // 2\n",
        "        self.mlp_gamma = nn.Conv2d(label_nc, norm_nc, kernel_size=ks, padding=pw)\n",
        "        self.mlp_beta = nn.Conv2d(label_nc, norm_nc, kernel_size=ks, padding=pw)\n",
        "\n",
        "    def forward(self, x, feat):\n",
        "        normalized = self.param_free_norm(x)\n",
        "        gamma = self.mlp_gamma(feat)\n",
        "        beta = self.mlp_beta(feat)\n",
        "        out = normalized * (1 + gamma) + beta\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "ul21ruLkbyZC"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# File   : replicate.py\n",
        "# Author : Jiayuan Mao\n",
        "# Email  : maojiayuan@gmail.com\n",
        "# Date   : 27/01/2018\n",
        "#\n",
        "# This file is part of Synchronized-BatchNorm-PyTorch.\n",
        "# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n",
        "# Distributed under MIT License.\n",
        "\n",
        "import functools\n",
        "\n",
        "from torch.nn.parallel.data_parallel import DataParallel\n",
        "\n",
        "__all__ = [\n",
        "    'CallbackContext',\n",
        "    'execute_replication_callbacks',\n",
        "    'DataParallelWithCallback',\n",
        "    'patch_replication_callback'\n",
        "]\n",
        "\n",
        "\n",
        "class CallbackContext(object):\n",
        "    pass\n",
        "\n",
        "\n",
        "def execute_replication_callbacks(modules):\n",
        "    \"\"\"\n",
        "    Execute an replication callback `__data_parallel_replicate__` on each module created by original replication.\n",
        "\n",
        "    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n",
        "\n",
        "    Note that, as all modules are isomorphism, we assign each sub-module with a context\n",
        "    (shared among multiple copies of this module on different devices).\n",
        "    Through this context, different copies can share some information.\n",
        "\n",
        "    We guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback\n",
        "    of any slave copies.\n",
        "    \"\"\"\n",
        "    master_copy = modules[0]\n",
        "    nr_modules = len(list(master_copy.modules()))\n",
        "    ctxs = [CallbackContext() for _ in range(nr_modules)]\n",
        "\n",
        "    for i, module in enumerate(modules):\n",
        "        for j, m in enumerate(module.modules()):\n",
        "            if hasattr(m, '__data_parallel_replicate__'):\n",
        "                m.__data_parallel_replicate__(ctxs[j], i)\n",
        "\n",
        "\n",
        "class DataParallelWithCallback(DataParallel):\n",
        "    \"\"\"\n",
        "    Data Parallel with a replication callback.\n",
        "\n",
        "    An replication callback `__data_parallel_replicate__` of each module will be invoked after being created by\n",
        "    original `replicate` function.\n",
        "    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n",
        "\n",
        "    Examples:\n",
        "        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n",
        "        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n",
        "        # sync_bn.__data_parallel_replicate__ will be invoked.\n",
        "    \"\"\"\n",
        "\n",
        "    def replicate(self, module, device_ids):\n",
        "        modules = super(DataParallelWithCallback, self).replicate(module, device_ids)\n",
        "        execute_replication_callbacks(modules)\n",
        "        return modules\n",
        "\n",
        "\n",
        "def patch_replication_callback(data_parallel):\n",
        "    \"\"\"\n",
        "    Monkey-patch an existing `DataParallel` object. Add the replication callback.\n",
        "    Useful when you have customized `DataParallel` implementation.\n",
        "\n",
        "    Examples:\n",
        "        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n",
        "        > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\n",
        "        > patch_replication_callback(sync_bn)\n",
        "        # this is equivalent to\n",
        "        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n",
        "        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n",
        "    \"\"\"\n",
        "\n",
        "    assert isinstance(data_parallel, DataParallel)\n",
        "\n",
        "    old_replicate = data_parallel.replicate\n",
        "\n",
        "    @functools.wraps(old_replicate)\n",
        "    def new_replicate(module, device_ids):\n",
        "        modules = old_replicate(module, device_ids)\n",
        "        execute_replication_callbacks(modules)\n",
        "        return modules\n",
        "\n",
        "    data_parallel.replicate = new_replicate"
      ],
      "metadata": {
        "id": "aj7ldxFFb0su"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class Stream(BaseNetwork):\n",
        "    def __init__(self, opt):\n",
        "        super().__init__()\n",
        "        self.opt = opt\n",
        "        nf = opt.ngf\n",
        "\n",
        "        # Initialize a ModuleList to store ResNet blocks for different resolution levels\n",
        "        self.res_blocks = nn.ModuleList()\n",
        "\n",
        "        # Loop to create ResNet blocks with increasing resolution\n",
        "        for i in range(8):\n",
        "            in_channels = nf * (2 ** (i - 1)) if i > 0 else opt.semantic_nc\n",
        "            out_channels = nf * (2 ** i)\n",
        "            self.res_blocks.append(StreamResnetBlock(in_channels, out_channels, opt))\n",
        "\n",
        "    def down(self, input):\n",
        "        # Downsample the input using bilinear interpolation\n",
        "        return F.interpolate(input, scale_factor=0.5)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Assume that input shape is (n, c, 256, 512)\n",
        "        outputs = []\n",
        "        x = input\n",
        "\n",
        "        # Loop through the ResNet blocks, downsampling and applying each block\n",
        "        for res_block in self.res_blocks:\n",
        "            x = self.down(x)\n",
        "            x = res_block(x)\n",
        "            outputs.append(x)\n",
        "\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "yqX8HYo5b1Q1"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "# File   : batchnorm.py\n",
        "# Author : Jiayuan Mao\n",
        "# Email  : maojiayuan@gmail.com\n",
        "# Date   : 27/01/2018\n",
        "#\n",
        "# This file is part of Synchronized-BatchNorm-PyTorch.\n",
        "# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n",
        "# Distributed under MIT License.\n",
        "\n",
        "import collections\n",
        "import contextlib\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "\n",
        "try:\n",
        "    from torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\n",
        "except ImportError:\n",
        "    ReduceAddCoalesced = Broadcast = None\n",
        "\n",
        "\n",
        "__all__ = [\n",
        "    'set_sbn_eps_mode',\n",
        "    'SynchronizedBatchNorm1d', 'SynchronizedBatchNorm2d', 'SynchronizedBatchNorm3d',\n",
        "    'patch_sync_batchnorm', 'convert_model'\n",
        "]\n",
        "\n",
        "\n",
        "SBN_EPS_MODE = 'clamp'\n",
        "\n",
        "\n",
        "def set_sbn_eps_mode(mode):\n",
        "    global SBN_EPS_MODE\n",
        "    assert mode in ('clamp', 'plus')\n",
        "    SBN_EPS_MODE = mode\n",
        "\n",
        "\n",
        "def _sum_ft(tensor):\n",
        "    \"\"\"sum over the first and last dimention\"\"\"\n",
        "    return tensor.sum(dim=0).sum(dim=-1)\n",
        "\n",
        "\n",
        "def _unsqueeze_ft(tensor):\n",
        "    \"\"\"add new dimensions at the front and the tail\"\"\"\n",
        "    return tensor.unsqueeze(0).unsqueeze(-1)\n",
        "\n",
        "\n",
        "_ChildMessage = collections.namedtuple('_ChildMessage', ['sum', 'ssum', 'sum_size'])\n",
        "_MasterMessage = collections.namedtuple('_MasterMessage', ['sum', 'inv_std'])\n",
        "\n",
        "\n",
        "class _SynchronizedBatchNorm(_BatchNorm):\n",
        "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True, track_running_stats=True):\n",
        "        assert ReduceAddCoalesced is not None, 'Can not use Synchronized Batch Normalization without CUDA support.'\n",
        "\n",
        "        super(_SynchronizedBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine,\n",
        "                                                     track_running_stats=track_running_stats)\n",
        "\n",
        "        if not self.track_running_stats:\n",
        "            import warnings\n",
        "            warnings.warn('track_running_stats=False is not supported by the SynchronizedBatchNorm.')\n",
        "\n",
        "        self._sync_master = SyncMaster(self._data_parallel_master)\n",
        "\n",
        "        self._is_parallel = False\n",
        "        self._parallel_id = None\n",
        "        self._slave_pipe = None\n",
        "\n",
        "    def forward(self, input):\n",
        "        # If it is not parallel computation or is in evaluation mode, use PyTorch's implementation.\n",
        "        if not (self._is_parallel and self.training):\n",
        "            return F.batch_norm(\n",
        "                input, self.running_mean, self.running_var, self.weight, self.bias,\n",
        "                self.training, self.momentum, self.eps)\n",
        "\n",
        "        # Resize the input to (B, C, -1).\n",
        "        input_shape = input.size()\n",
        "        assert input.size(1) == self.num_features, 'Channel size mismatch: got {}, expect {}.'.format(input.size(1), self.num_features)\n",
        "        input = input.view(input.size(0), self.num_features, -1)\n",
        "\n",
        "        # Compute the sum and square-sum.\n",
        "        sum_size = input.size(0) * input.size(2)\n",
        "        input_sum = _sum_ft(input)\n",
        "        input_ssum = _sum_ft(input ** 2)\n",
        "\n",
        "        # Reduce-and-broadcast the statistics.\n",
        "        if self._parallel_id == 0:\n",
        "            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n",
        "        else:\n",
        "            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n",
        "\n",
        "        # Compute the output.\n",
        "        if self.affine:\n",
        "            # MJY:: Fuse the multiplication for speed.\n",
        "            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n",
        "        else:\n",
        "            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n",
        "\n",
        "        # Reshape it.\n",
        "        return output.view(input_shape)\n",
        "\n",
        "    def __data_parallel_replicate__(self, ctx, copy_id):\n",
        "        self._is_parallel = True\n",
        "        self._parallel_id = copy_id\n",
        "\n",
        "        # parallel_id == 0 means master device.\n",
        "        if self._parallel_id == 0:\n",
        "            ctx.sync_master = self._sync_master\n",
        "        else:\n",
        "            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n",
        "\n",
        "    def _data_parallel_master(self, intermediates):\n",
        "        \"\"\"Reduce the sum and square-sum, compute the statistics, and broadcast it.\"\"\"\n",
        "\n",
        "        # Always using same \"device order\" makes the ReduceAdd operation faster.\n",
        "        # Thanks to:: Tete Xiao (http://tetexiao.com/)\n",
        "        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n",
        "\n",
        "        to_reduce = [i[1][:2] for i in intermediates]\n",
        "        to_reduce = [j for i in to_reduce for j in i]  # flatten\n",
        "        target_gpus = [i[1].sum.get_device() for i in intermediates]\n",
        "\n",
        "        sum_size = sum([i[1].sum_size for i in intermediates])\n",
        "        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n",
        "        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n",
        "\n",
        "        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n",
        "\n",
        "        outputs = []\n",
        "        for i, rec in enumerate(intermediates):\n",
        "            outputs.append((rec[0], _MasterMessage(*broadcasted[i*2:i*2+2])))\n",
        "\n",
        "        return outputs\n",
        "\n",
        "    def _compute_mean_std(self, sum_, ssum, size):\n",
        "        \"\"\"Compute the mean and standard-deviation with sum and square-sum. This method\n",
        "        also maintains the moving average on the master device.\"\"\"\n",
        "        assert size > 1, 'BatchNorm computes unbiased standard-deviation, which requires size > 1.'\n",
        "        mean = sum_ / size\n",
        "        sumvar = ssum - sum_ * mean\n",
        "        unbias_var = sumvar / (size - 1)\n",
        "        bias_var = sumvar / size\n",
        "\n",
        "        if hasattr(torch, 'no_grad'):\n",
        "            with torch.no_grad():\n",
        "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\n",
        "                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\n",
        "        else:\n",
        "            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\n",
        "            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\n",
        "\n",
        "        if SBN_EPS_MODE == 'clamp':\n",
        "            return mean, bias_var.clamp(self.eps) ** -0.5\n",
        "        elif SBN_EPS_MODE == 'plus':\n",
        "            return mean, (bias_var + self.eps) ** -0.5\n",
        "        else:\n",
        "            raise ValueError('Unknown EPS mode: {}.'.format(SBN_EPS_MODE))\n",
        "\n",
        "\n",
        "class SynchronizedBatchNorm1d(_SynchronizedBatchNorm):\n",
        "    r\"\"\"Applies Synchronized Batch Normalization over a 2d or 3d input that is seen as a\n",
        "    mini-batch.\n",
        "\n",
        "    .. math::\n",
        "\n",
        "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
        "\n",
        "    This module differs from the built-in PyTorch BatchNorm1d as the mean and\n",
        "    standard-deviation are reduced across all devices during training.\n",
        "\n",
        "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
        "    training, PyTorch's implementation normalize the tensor on each device using\n",
        "    the statistics only on that device, which accelerated the computation and\n",
        "    is also easy to implement, but the statistics might be inaccurate.\n",
        "    Instead, in this synchronized version, the statistics will be computed\n",
        "    over all training samples distributed on multiple devices.\n",
        "\n",
        "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
        "    as the built-in PyTorch implementation.\n",
        "\n",
        "    The mean and standard-deviation are calculated per-dimension over\n",
        "    the mini-batches and gamma and beta are learnable parameter vectors\n",
        "    of size C (where C is the input size).\n",
        "\n",
        "    During training, this layer keeps a running estimate of its computed mean\n",
        "    and variance. The running sum is kept with a default momentum of 0.1.\n",
        "\n",
        "    During evaluation, this running mean/variance is used for normalization.\n",
        "\n",
        "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
        "    on `(N, L)` slices, it's common terminology to call this Temporal BatchNorm\n",
        "\n",
        "    Args:\n",
        "        num_features: num_features from an expected input of size\n",
        "            `batch_size x num_features [x width]`\n",
        "        eps: a value added to the denominator for numerical stability.\n",
        "            Default: 1e-5\n",
        "        momentum: the value used for the running_mean and running_var\n",
        "            computation. Default: 0.1\n",
        "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
        "            affine parameters. Default: ``True``\n",
        "\n",
        "    Shape::\n",
        "        - Input: :math:`(N, C)` or :math:`(N, C, L)`\n",
        "        - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n",
        "\n",
        "    Examples:\n",
        "        >>> # With Learnable Parameters\n",
        "        >>> m = SynchronizedBatchNorm1d(100)\n",
        "        >>> # Without Learnable Parameters\n",
        "        >>> m = SynchronizedBatchNorm1d(100, affine=False)\n",
        "        >>> input = torch.autograd.Variable(torch.randn(20, 100))\n",
        "        >>> output = m(input)\n",
        "    \"\"\"\n",
        "\n",
        "    def _check_input_dim(self, input):\n",
        "        if input.dim() != 2 and input.dim() != 3:\n",
        "            raise ValueError('expected 2D or 3D input (got {}D input)'\n",
        "                             .format(input.dim()))\n",
        "\n",
        "\n",
        "class SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n",
        "    r\"\"\"Applies Batch Normalization over a 4d input that is seen as a mini-batch\n",
        "    of 3d inputs\n",
        "\n",
        "    .. math::\n",
        "\n",
        "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
        "\n",
        "    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n",
        "    standard-deviation are reduced across all devices during training.\n",
        "\n",
        "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
        "    training, PyTorch's implementation normalize the tensor on each device using\n",
        "    the statistics only on that device, which accelerated the computation and\n",
        "    is also easy to implement, but the statistics might be inaccurate.\n",
        "    Instead, in this synchronized version, the statistics will be computed\n",
        "    over all training samples distributed on multiple devices.\n",
        "\n",
        "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
        "    as the built-in PyTorch implementation.\n",
        "\n",
        "    The mean and standard-deviation are calculated per-dimension over\n",
        "    the mini-batches and gamma and beta are learnable parameter vectors\n",
        "    of size C (where C is the input size).\n",
        "\n",
        "    During training, this layer keeps a running estimate of its computed mean\n",
        "    and variance. The running sum is kept with a default momentum of 0.1.\n",
        "\n",
        "    During evaluation, this running mean/variance is used for normalization.\n",
        "\n",
        "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
        "    on `(N, H, W)` slices, it's common terminology to call this Spatial BatchNorm\n",
        "\n",
        "    Args:\n",
        "        num_features: num_features from an expected input of\n",
        "            size batch_size x num_features x height x width\n",
        "        eps: a value added to the denominator for numerical stability.\n",
        "            Default: 1e-5\n",
        "        momentum: the value used for the running_mean and running_var\n",
        "            computation. Default: 0.1\n",
        "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
        "            affine parameters. Default: ``True``\n",
        "\n",
        "    Shape::\n",
        "        - Input: :math:`(N, C, H, W)`\n",
        "        - Output: :math:`(N, C, H, W)` (same shape as input)\n",
        "\n",
        "    Examples:\n",
        "        >>> # With Learnable Parameters\n",
        "        >>> m = SynchronizedBatchNorm2d(100)\n",
        "        >>> # Without Learnable Parameters\n",
        "        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n",
        "        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n",
        "        >>> output = m(input)\n",
        "    \"\"\"\n",
        "\n",
        "    def _check_input_dim(self, input):\n",
        "        if input.dim() != 4:\n",
        "            raise ValueError('expected 4D input (got {}D input)'\n",
        "                             .format(input.dim()))\n",
        "\n",
        "\n",
        "class SynchronizedBatchNorm3d(_SynchronizedBatchNorm):\n",
        "    r\"\"\"Applies Batch Normalization over a 5d input that is seen as a mini-batch\n",
        "    of 4d inputs\n",
        "\n",
        "    .. math::\n",
        "\n",
        "        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n",
        "\n",
        "    This module differs from the built-in PyTorch BatchNorm3d as the mean and\n",
        "    standard-deviation are reduced across all devices during training.\n",
        "\n",
        "    For example, when one uses `nn.DataParallel` to wrap the network during\n",
        "    training, PyTorch's implementation normalize the tensor on each device using\n",
        "    the statistics only on that device, which accelerated the computation and\n",
        "    is also easy to implement, but the statistics might be inaccurate.\n",
        "    Instead, in this synchronized version, the statistics will be computed\n",
        "    over all training samples distributed on multiple devices.\n",
        "\n",
        "    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n",
        "    as the built-in PyTorch implementation.\n",
        "\n",
        "    The mean and standard-deviation are calculated per-dimension over\n",
        "    the mini-batches and gamma and beta are learnable parameter vectors\n",
        "    of size C (where C is the input size).\n",
        "\n",
        "    During training, this layer keeps a running estimate of its computed mean\n",
        "    and variance. The running sum is kept with a default momentum of 0.1.\n",
        "\n",
        "    During evaluation, this running mean/variance is used for normalization.\n",
        "\n",
        "    Because the BatchNorm is done over the `C` dimension, computing statistics\n",
        "    on `(N, D, H, W)` slices, it's common terminology to call this Volumetric BatchNorm\n",
        "    or Spatio-temporal BatchNorm\n",
        "\n",
        "    Args:\n",
        "        num_features: num_features from an expected input of\n",
        "            size batch_size x num_features x depth x height x width\n",
        "        eps: a value added to the denominator for numerical stability.\n",
        "            Default: 1e-5\n",
        "        momentum: the value used for the running_mean and running_var\n",
        "            computation. Default: 0.1\n",
        "        affine: a boolean value that when set to ``True``, gives the layer learnable\n",
        "            affine parameters. Default: ``True``\n",
        "\n",
        "    Shape::\n",
        "        - Input: :math:`(N, C, D, H, W)`\n",
        "        - Output: :math:`(N, C, D, H, W)` (same shape as input)\n",
        "\n",
        "    Examples:\n",
        "        >>> # With Learnable Parameters\n",
        "        >>> m = SynchronizedBatchNorm3d(100)\n",
        "        >>> # Without Learnable Parameters\n",
        "        >>> m = SynchronizedBatchNorm3d(100, affine=False)\n",
        "        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45, 10))\n",
        "        >>> output = m(input)\n",
        "    \"\"\"\n",
        "\n",
        "    def _check_input_dim(self, input):\n",
        "        if input.dim() != 5:\n",
        "            raise ValueError('expected 5D input (got {}D input)'\n",
        "                             .format(input.dim()))\n",
        "\n",
        "\n",
        "@contextlib.contextmanager\n",
        "def patch_sync_batchnorm():\n",
        "    import torch.nn as nn\n",
        "\n",
        "    backup = nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d\n",
        "\n",
        "    nn.BatchNorm1d = SynchronizedBatchNorm1d\n",
        "    nn.BatchNorm2d = SynchronizedBatchNorm2d\n",
        "    nn.BatchNorm3d = SynchronizedBatchNorm3d\n",
        "\n",
        "    yield\n",
        "\n",
        "    nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d = backup\n",
        "\n",
        "\n",
        "def convert_model(module):\n",
        "    \"\"\"Traverse the input module and its child recursively\n",
        "       and replace all instance of torch.nn.modules.batchnorm.BatchNorm*N*d\n",
        "       to SynchronizedBatchNorm*N*d\n",
        "\n",
        "    Args:\n",
        "        module: the input module needs to be convert to SyncBN model\n",
        "\n",
        "    Examples:\n",
        "        >>> import torch.nn as nn\n",
        "        >>> import torchvision\n",
        "        >>> # m is a standard pytorch model\n",
        "        >>> m = torchvision.models.resnet18(True)\n",
        "        >>> m = nn.DataParallel(m)\n",
        "        >>> # after convert, m is using SyncBN\n",
        "        >>> m = convert_model(m)\n",
        "    \"\"\"\n",
        "    if isinstance(module, torch.nn.DataParallel):\n",
        "        mod = module.module\n",
        "        mod = convert_model(mod)\n",
        "        mod = DataParallelWithCallback(mod, device_ids=module.device_ids)\n",
        "        return mod\n",
        "\n",
        "    mod = module\n",
        "    for pth_module, sync_module in zip([torch.nn.modules.batchnorm.BatchNorm1d,\n",
        "                                        torch.nn.modules.batchnorm.BatchNorm2d,\n",
        "                                        torch.nn.modules.batchnorm.BatchNorm3d],\n",
        "                                       [SynchronizedBatchNorm1d,\n",
        "                                        SynchronizedBatchNorm2d,\n",
        "                                        SynchronizedBatchNorm3d]):\n",
        "        if isinstance(module, pth_module):\n",
        "            mod = sync_module(module.num_features, module.eps, module.momentum, module.affine)\n",
        "            mod.running_mean = module.running_mean\n",
        "            mod.running_var = module.running_var\n",
        "            if module.affine:\n",
        "                mod.weight.data = module.weight.data.clone().detach()\n",
        "                mod.bias.data = module.bias.data.clone().detach()\n",
        "\n",
        "    for name, child in module.named_children():\n",
        "        mod.add_module(name, convert_model(child))\n",
        "\n",
        "    return mod"
      ],
      "metadata": {
        "id": "IlreAW35b2o5"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "import torch\n",
        "\n",
        "def find_model_using_name(model_name):\n",
        "    # Given the option --model [modelname],\n",
        "    # the file \"models/modelname_model.py\"\n",
        "    # will be imported.\n",
        "    model_filename = model_name + \"_model\"\n",
        "\n",
        "    modellib = importlib.import_module(model_filename)\n",
        "\n",
        "    # In the file, the class called ModelNameModel() will\n",
        "    # be instantiated. It has to be a subclass of torch.nn.Module,\n",
        "    # and it is case-insensitive.\n",
        "    target_model_name = model_name.replace('_', '') + 'model'\n",
        "    model_cls = getattr(modellib, target_model_name, None)\n",
        "\n",
        "    if model_cls is None or not issubclass(model_cls, torch.nn.Module):\n",
        "        raise ImportError(f\"In {model_filename}.py, there should be a subclass of torch.nn.Module with class name that matches {target_model_name} in lowercase.\")\n",
        "\n",
        "    return model_cls\n",
        "\n",
        "def get_option_setter(model_name):\n",
        "    # Returns the command line options modification function\n",
        "    # for the specified model.\n",
        "    model_class = find_model_using_name(model_name)\n",
        "    return model_class.modify_commandline_options\n",
        "\n",
        "def create_model(opt):\n",
        "    # Creates an instance of the specified model.\n",
        "    model_cls = find_model_using_name(opt.model)\n",
        "\n",
        "    try:\n",
        "        instance = model_cls(opt)\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"Failed to create an instance of {model_cls.__name__}. Error: {e}\")\n",
        "\n",
        "    print(f\"Model [{model_cls.__name__}] was created\")\n",
        "\n",
        "    return instance\n"
      ],
      "metadata": {
        "id": "6jvFSII4b7ky"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class pix2pixmodel(torch.nn.Module):\n",
        "    @staticmethod\n",
        "    def modify_commandline_options(parser, is_train):\n",
        "        modify_commandline_options(parser, is_train)\n",
        "        return parser\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super().__init__()\n",
        "        self.opt = opt\n",
        "        self.FloatTensor = torch.cuda.FloatTensor if self.use_gpu() \\\n",
        "            else torch.FloatTensor\n",
        "        self.ByteTensor = torch.cuda.ByteTensor if self.use_gpu() \\\n",
        "            else torch.ByteTensor\n",
        "\n",
        "        self.netG, self.netD, self.netE = self.initialize_networks(opt)\n",
        "\n",
        "        # set loss functions\n",
        "        if opt.isTrain:\n",
        "            self.criterionGAN = GANLoss(\n",
        "                opt.gan_mode, tensor=self.FloatTensor, opt=self.opt)\n",
        "            self.criterionFeat = torch.nn.L1Loss()\n",
        "            if not opt.no_vgg_loss:\n",
        "                self.criterionVGG = VGGLoss(self.opt.gpu_ids)\n",
        "            if opt.use_vae:\n",
        "                self.KLDLoss = KLDLoss()\n",
        "\n",
        "    # Entry point for all calls involving forward pass\n",
        "    # of deep networks. We used this approach since DataParallel module\n",
        "    # can't parallelize custom functions, we branch to different\n",
        "    # routines based on |mode|.\n",
        "    def forward(self, data, mode):\n",
        "        input_semantics, real_image = self.preprocess_input(data)\n",
        "        if mode == 'generator':\n",
        "            g_loss, generated = self.compute_generator_loss(\n",
        "                input_semantics, real_image)\n",
        "            return g_loss, generated\n",
        "        elif mode == 'discriminator':\n",
        "            d_loss = self.compute_discriminator_loss(\n",
        "                input_semantics, real_image)\n",
        "            return d_loss\n",
        "        elif mode == 'encode_only':\n",
        "            z, mu, logvar = self.encode_z(real_image)\n",
        "            return mu, logvar\n",
        "        elif mode == 'inference':\n",
        "            with torch.no_grad():\n",
        "                fake_image, _ = self.generate_fake(input_semantics, real_image)\n",
        "            return fake_image\n",
        "        else:\n",
        "            raise ValueError(\"|mode| is invalid\")\n",
        "\n",
        "    def create_optimizers(self, opt):\n",
        "        G_params = list(self.netG.parameters())\n",
        "        if opt.use_vae:\n",
        "            G_params += list(self.netE.parameters())\n",
        "        if opt.isTrain:\n",
        "            D_params = list(self.netD.parameters())\n",
        "\n",
        "        if opt.no_TTUR:\n",
        "            beta1, beta2 = opt.beta1, opt.beta2\n",
        "            G_lr, D_lr = opt.lr, opt.lr\n",
        "        else:\n",
        "            beta1, beta2 = 0, 0.9\n",
        "            G_lr, D_lr = opt.lr / 2, opt.lr * 2\n",
        "\n",
        "        optimizer_G = torch.optim.Adam(G_params, lr=G_lr, betas=(beta1, beta2))\n",
        "        optimizer_D = torch.optim.Adam(D_params, lr=D_lr, betas=(beta1, beta2))\n",
        "\n",
        "        return {'optimizer_G': optimizer_G, 'optimizer_D': optimizer_D}\n",
        "\n",
        "\n",
        "    def save(self, epoch):\n",
        "        save_network(self.netG, 'G', epoch, self.opt)\n",
        "        save_network(self.netD, 'D', epoch, self.opt)\n",
        "        if self.opt.use_vae:\n",
        "            save_network(self.netE, 'E', epoch, self.opt)\n",
        "\n",
        "    ############################################################################\n",
        "    # Private helper methods\n",
        "    ############################################################################\n",
        "\n",
        "    def initialize_networks(self, opt):\n",
        "        netG = define_G(opt)\n",
        "        netD = define_D(opt) if opt.isTrain else None\n",
        "        netE = define_E(opt) if opt.use_vae else None\n",
        "\n",
        "        if not opt.isTrain or opt.continue_train:\n",
        "            netG = load_network(netG, 'G', opt.which_epoch, opt)\n",
        "            if opt.isTrain:\n",
        "                netD = load_network(netD, 'D', opt.which_epoch, opt)\n",
        "            if opt.use_vae:\n",
        "                netE = load_network(netE, 'E', opt.which_epoch, opt)\n",
        "\n",
        "        return netG, netD, netE\n",
        "\n",
        "    # preprocess the input, such as moving the tensors to GPUs\n",
        "    # and transforming the label map to one-hot encoding (for SIS)\n",
        "    # |data|: dictionary of the input data\n",
        "    def preprocess_input(self, data):\n",
        "        # move to GPU and change data types\n",
        "        if self.opt.task == 'SIS':\n",
        "            data['label'] = data['label'].long()\n",
        "        if self.use_gpu():\n",
        "            data['label'] = data['label'].cuda()\n",
        "            data['instance'] = data['instance'].cuda()\n",
        "            data['image'] = data['image'].cuda()\n",
        "\n",
        "        # create one-hot label map for SIS\n",
        "        if self.opt.task == 'SIS':\n",
        "            label_map = data['label']\n",
        "            bs, _, h, w = label_map.size()\n",
        "            nc = self.opt.label_nc + 1 if self.opt.contain_dontcare_label \\\n",
        "                else self.opt.label_nc\n",
        "            input_label = self.FloatTensor(bs, nc, h, w).zero_()\n",
        "            input_semantics = input_label.scatter_(1, label_map, 1.0)\n",
        "\n",
        "            # concatenate instance map if it exists\n",
        "            if not self.opt.no_instance:\n",
        "                inst_map = data['instance']\n",
        "                instance_edge_map = self.get_edges(inst_map)\n",
        "                input_semantics = torch.cat((input_semantics, instance_edge_map), dim=1)\n",
        "        else:\n",
        "            input_semantics = data['label']\n",
        "\n",
        "        return input_semantics, data['image']\n",
        "\n",
        "    def compute_generator_loss(self, content, style):\n",
        "        G_losses = {}\n",
        "\n",
        "        fake_image, KLD_loss = self.generate_fake(\n",
        "            content, style, compute_kld_loss=self.opt.use_vae)\n",
        "\n",
        "        if self.opt.use_vae:\n",
        "            G_losses['KLD'] = KLD_loss\n",
        "\n",
        "        if self.opt.task == 'SIS':\n",
        "            pred_fake, pred_real = self.discriminate(fake_image, style, content)\n",
        "        else:\n",
        "            pred_fake, pred_real = self.discriminate(fake_image, style)\n",
        "\n",
        "        G_losses['GAN'] = self.criterionGAN(pred_fake, True,\n",
        "                                            for_discriminator=False)\n",
        "\n",
        "        if not self.opt.no_ganFeat_loss:\n",
        "            num_D = len(pred_fake)\n",
        "            GAN_Feat_loss = self.FloatTensor(1).fill_(0)\n",
        "            for i in range(num_D):  # for each discriminator\n",
        "                # last output is the final prediction, so we exclude it\n",
        "                num_intermediate_outputs = len(pred_fake[i]) - 1\n",
        "                for j in range(num_intermediate_outputs):  # for each layer output\n",
        "                    unweighted_loss = self.criterionFeat(\n",
        "                        pred_fake[i][j], pred_real[i][j].detach())\n",
        "                    GAN_Feat_loss += unweighted_loss * self.opt.lambda_feat / num_D\n",
        "            G_losses['GAN_Feat'] = GAN_Feat_loss\n",
        "\n",
        "        if not self.opt.no_vgg_loss:\n",
        "            target = style if self.opt.task == 'SIS' else content\n",
        "            G_losses['VGG'] = self.criterionVGG(fake_image, target) * self.opt.lambda_vgg\n",
        "\n",
        "        return G_losses, fake_image\n",
        "\n",
        "    def compute_discriminator_loss(self, content, style):\n",
        "        D_losses = {}\n",
        "        with torch.no_grad():\n",
        "            fake_image, _ = self.generate_fake(content, style)\n",
        "            fake_image = fake_image.detach()\n",
        "            fake_image.requires_grad_()\n",
        "\n",
        "        if self.opt.task == 'SIS':\n",
        "            pred_fake, pred_real = self.discriminate(fake_image, style, content)\n",
        "        else:\n",
        "            pred_fake, pred_real = self.discriminate(fake_image, style)\n",
        "\n",
        "        D_losses['D_Fake'] = self.criterionGAN(pred_fake, False,\n",
        "                                               for_discriminator=True)\n",
        "        D_losses['D_real'] = self.criterionGAN(pred_real, True,\n",
        "                                               for_discriminator=True)\n",
        "\n",
        "        return D_losses\n",
        "\n",
        "    def encode_z(self, real_image):\n",
        "        mu, logvar = self.netE(real_image)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return z, mu, logvar\n",
        "\n",
        "    def generate_fake(self, input_semantics, real_image, compute_kld_loss=False):\n",
        "        z = None\n",
        "        KLD_loss = None\n",
        "        if self.opt.use_vae:\n",
        "            z, mu, logvar = self.encode_z(real_image)\n",
        "            if compute_kld_loss:\n",
        "                KLD_loss = self.KLDLoss(mu, logvar) * self.opt.lambda_kld\n",
        "\n",
        "        fake_image = self.netG(input_semantics, real_image, z=z)\n",
        "\n",
        "        assert (not compute_kld_loss) or self.opt.use_vae, \\\n",
        "            \"You cannot compute KLD loss if opt.use_vae == False\"\n",
        "\n",
        "        return fake_image, KLD_loss\n",
        "\n",
        "    # Given fake and real image, return the prediction of discriminator\n",
        "    # for each fake and real image. The condition is used in SIS.\n",
        "    def discriminate(self, fake_image, real_image, condition=None):\n",
        "        if self.opt.task == 'SIS':\n",
        "            assert condition is not None\n",
        "            fake_concat = torch.cat([condition, fake_image], dim=1)\n",
        "            real_concat = torch.cat([condition, real_image], dim=1)\n",
        "        else:\n",
        "            assert condition is None\n",
        "            fake_concat = fake_image\n",
        "            real_concat = real_image\n",
        "\n",
        "        # In Batch Normalization, the fake and real images are\n",
        "        # recommended to be in the same batch to avoid disparate\n",
        "        # statistics in fake and real images.\n",
        "        # So both fake and real images are fed to D all at once.\n",
        "        fake_and_real = torch.cat([fake_concat, real_concat], dim=0)\n",
        "\n",
        "        discriminator_out = self.netD(fake_and_real)\n",
        "\n",
        "        pred_fake, pred_real = self.divide_pred(discriminator_out)\n",
        "\n",
        "        return pred_fake, pred_real\n",
        "\n",
        "    # Take the prediction of fake and real images from the combined batch\n",
        "    def divide_pred(self, pred):\n",
        "        # the prediction contains the intermediate outputs of multi-scale GAN,\n",
        "        # so it's usually a list\n",
        "        if type(pred) == list:\n",
        "            fake = []\n",
        "            real = []\n",
        "            for p in pred:\n",
        "                fake.append([tensor[:tensor.size(0) // 2] for tensor in p])\n",
        "                real.append([tensor[tensor.size(0) // 2:] for tensor in p])\n",
        "        else:\n",
        "            fake = pred[:pred.size(0) // 2]\n",
        "            real = pred[pred.size(0) // 2:]\n",
        "\n",
        "        return fake, real\n",
        "\n",
        "    def get_edges(self, t):\n",
        "        edge = self.ByteTensor(t.size()).zero_()\n",
        "        edge[:, :, :, 1:] = edge[:, :, :, 1:] | (t[:, :, :, 1:] != t[:, :, :, :-1])\n",
        "        edge[:, :, :, :-1] = edge[:, :, :, :-1] | (t[:, :, :, 1:] != t[:, :, :, :-1])\n",
        "        edge[:, :, 1:, :] = edge[:, :, 1:, :] | (t[:, :, 1:, :] != t[:, :, :-1, :])\n",
        "        edge[:, :, :-1, :] = edge[:, :, :-1, :] | (t[:, :, 1:, :] != t[:, :, :-1, :])\n",
        "        return edge.float()\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std) + mu\n",
        "\n",
        "    def use_gpu(self):\n",
        "        return len(self.opt.gpu_ids) > 0"
      ],
      "metadata": {
        "id": "vRcfZrJPb8UC"
      },
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "import pickle\n",
        "\n",
        "\n",
        "class BaseOptions():\n",
        "    def __init__(self):\n",
        "        self.initialized = False\n",
        "\n",
        "    def initialize(self, parser):\n",
        "        # experiment specifics\n",
        "        parser.add_argument('--name', type=str, default='ast_summer2winteryosemite', help='name of the experiment. It decides where to store samples and models')\n",
        "        parser.add_argument('--task', type=str, default='AST', help='task type: AST | SIS | MMIS')\n",
        "\n",
        "        parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n",
        "        parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n",
        "        parser.add_argument('--model', type=str, default='pix2pix', help='which model to use')\n",
        "        parser.add_argument('--norm_G', type=str, default='spectralinstance', help='instance normalization or batch normalization')\n",
        "        parser.add_argument('--norm_D', type=str, default='spectralinstance', help='instance normalization or batch normalization')\n",
        "        parser.add_argument('--norm_S', type=str, default='spectralinstance', help='instance normalization or batch normalization')\n",
        "        parser.add_argument('--norm_E', type=str, default='spectralinstance', help='instance normalization or batch normalization')\n",
        "        parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n",
        "\n",
        "        # input/output sizes\n",
        "        parser.add_argument('--batchSize', type=int, default=1, help='input batch size')\n",
        "        parser.add_argument('--preprocess_mode', type=str, default='scale_width_and_crop', help='scaling and cropping of images at load time.', choices=(\"resize_and_crop\", \"crop\", \"scale_width\", \"scale_width_and_crop\", \"scale_shortside\", \"scale_shortside_and_crop\", \"fixed\", \"none\"))\n",
        "        parser.add_argument('--load_size', type=int, default=1024, help='Scale images to this size. The final image will be cropped to --crop_size.')\n",
        "        parser.add_argument('--crop_size', type=int, default=512, help='Crop to the width of crop_size (after initially scaling the images to load_size.)')\n",
        "        parser.add_argument('--aspect_ratio', type=float, default=1.0, help='The ratio width/height. The final height of the load image will be crop_size/aspect_ratio')\n",
        "        parser.add_argument('--label_nc', type=int, default=3, help='# of input label classes without unknown class. If you have unknown class as class label, specify --contain_dopntcare_label.')\n",
        "        parser.add_argument('--contain_dontcare_label', action='store_true', help='if the label map contains dontcare label (dontcare=255)')\n",
        "        parser.add_argument('--output_nc', type=int, default=3, help='# of output image channels')\n",
        "\n",
        "        # for setting inputs\n",
        "        parser.add_argument('--dataset_mode', type=str, default='summer2winteryosemite')\n",
        "        parser.add_argument('--croot', type=str, default='./datasets/summer2winter_yosemite/', help='content dataroot')\n",
        "        parser.add_argument('--sroot', type=str, default='./datasets/summer2winter_yosemite/', help='style dataroot')\n",
        "        parser.add_argument('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')\n",
        "        parser.add_argument('--no_flip', action='store_true', help='if specified, do not flip the images for data argumentation')\n",
        "        parser.add_argument('--nThreads', default=0, type=int, help='# threads for loading data')\n",
        "        parser.add_argument('--max_dataset_size', type=int, default=sys.maxsize, help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')\n",
        "        parser.add_argument('--load_from_opt_file', action='store_true', help='load the options from checkpoints and use that as default')\n",
        "        parser.add_argument('--cache_filelist_write', action='store_true', help='saves the current filelist into a text file, so that it loads faster')\n",
        "        parser.add_argument('--cache_filelist_read', action='store_true', help='reads from the file list cache')\n",
        "\n",
        "        # for displays\n",
        "        parser.add_argument('--display_winsize', type=int, default=400, help='display window size')\n",
        "\n",
        "        # for generator\n",
        "        parser.add_argument('--netG', type=str, default='tsit', help='selects model to use for netG (tsit | pix2pixhd)')\n",
        "        parser.add_argument('--ngf', type=int, default=64, help='# of gen filters in first conv layer')\n",
        "        parser.add_argument('--init_type', type=str, default='xavier', help='network initialization [normal|xavier|kaiming|orthogonal]')\n",
        "        parser.add_argument('--init_variance', type=float, default=0.02, help='variance of the initialization distribution')\n",
        "        parser.add_argument('--z_dim', type=int, default=256, help=\"dimension of the latent z vector\")\n",
        "        parser.add_argument('--alpha', type=float, default=1.0, help='The parameter that controls the degree of stylization (between 0 and 1)')\n",
        "        parser.add_argument('--no_ss', action='store_true', help='discard the style stream (better results in certain cases).')\n",
        "\n",
        "        # for instance-wise features\n",
        "        parser.add_argument('--no_instance', action='store_true', help='if specified, do *not* add instance map as input')\n",
        "        parser.add_argument('--nef', type=int, default=16, help='# of encoder filters in the first conv layer')\n",
        "        parser.add_argument('--use_vae', action='store_true', help='enable training with an image encoder.')\n",
        "\n",
        "        self.initialized = True\n",
        "\n",
        "        return parser\n",
        "\n",
        "    def gather_options(self):\n",
        "        # initialize parser with basic options\n",
        "        if not self.initialized:\n",
        "            parser = argparse.ArgumentParser(\n",
        "                formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "            parser = self.initialize(parser)\n",
        "\n",
        "        # get the basic options\n",
        "        opt, unknown = parser.parse_known_args()\n",
        "\n",
        "        # modify model-related parser options\n",
        "        model_name = opt.model\n",
        "        model_option_setter = get_option_setter(model_name)\n",
        "        parser = model_option_setter(parser, self.isTrain)\n",
        "\n",
        "        # modify dataset-related parser options\n",
        "        dataset_mode = opt.dataset_mode\n",
        "        dataset_option_setter = data.get_option_setter(dataset_mode)\n",
        "        parser = dataset_option_setter(parser, self.isTrain)\n",
        "\n",
        "        opt, unknown = parser.parse_known_args()\n",
        "\n",
        "        # if there is opt_file, load it.\n",
        "        # The previous default options will be overwritten\n",
        "        if opt.load_from_opt_file:\n",
        "            parser = self.update_options_from_file(parser, opt)\n",
        "\n",
        "        opt = parser.parse_args()\n",
        "        self.parser = parser\n",
        "        return opt\n",
        "\n",
        "    def print_options(self, opt):\n",
        "        message = ''\n",
        "        message += '----------------- Options ---------------\\n'\n",
        "        for k, v in sorted(vars(opt).items()):\n",
        "            comment = ''\n",
        "            default = self.parser.get_default(k)\n",
        "            if v != default:\n",
        "                comment = '\\t[default: %s]' % str(default)\n",
        "            message += '{:>25}: {:<30}{}\\n'.format(str(k), str(v), comment)\n",
        "        message += '----------------- End -------------------'\n",
        "        print(message)\n",
        "\n",
        "    def option_file_path(self, opt, makedir=False):\n",
        "        expr_dir = os.path.join(opt.checkpoints_dir, opt.name)\n",
        "        if makedir:\n",
        "            mkdirs(expr_dir)\n",
        "        file_name = os.path.join(expr_dir, 'opt')\n",
        "        return file_name\n",
        "\n",
        "    def save_options(self, opt):\n",
        "        file_name = self.option_file_path(opt, makedir=True)\n",
        "        with open(file_name + '.txt', 'wt') as opt_file:\n",
        "            for k, v in sorted(vars(opt).items()):\n",
        "                comment = ''\n",
        "                default = self.parser.get_default(k)\n",
        "                if v != default:\n",
        "                    comment = '\\t[default: %s]' % str(default)\n",
        "                opt_file.write('{:>25}: {:<30}{}\\n'.format(str(k), str(v), comment))\n",
        "\n",
        "        with open(file_name + '.pkl', 'wb') as opt_file:\n",
        "            pickle.dump(opt, opt_file)\n",
        "\n",
        "    def update_options_from_file(self, parser, opt):\n",
        "        new_opt = self.load_options(opt)\n",
        "        for k, v in sorted(vars(opt).items()):\n",
        "            if hasattr(new_opt, k) and v != getattr(new_opt, k):\n",
        "                new_val = getattr(new_opt, k)\n",
        "                parser.set_defaults(**{k: new_val})\n",
        "        return parser\n",
        "\n",
        "    def load_options(self, opt):\n",
        "        file_name = self.option_file_path(opt, makedir=False)\n",
        "        new_opt = pickle.load(open(file_name + '.pkl', 'rb'))\n",
        "        return new_opt\n",
        "\n",
        "    def parse(self, save=False):\n",
        "\n",
        "        opt = self.gather_options()\n",
        "        opt.num_upsampling_layers = 'more'\n",
        "        opt.isTrain = self.isTrain   # train or test\n",
        "        assert opt.task == 'AST' or opt.task == 'SIS' or opt.task == 'MMIS', \\\n",
        "            f'Task type should be: AST | SIS | MMIS, but got {opt.task}.'\n",
        "\n",
        "        # Set semantic_nc based on the option.\n",
        "        # This will be convenient in many places\n",
        "        if opt.task == 'SIS':\n",
        "            opt.semantic_nc = opt.label_nc + \\\n",
        "                              (1 if opt.contain_dontcare_label else 0) + \\\n",
        "                              (0 if opt.no_instance else 1)\n",
        "            opt.no_ss = True\n",
        "        else:\n",
        "            opt.semantic_nc = 3\n",
        "\n",
        "        self.print_options(opt)\n",
        "        if opt.isTrain:\n",
        "            self.save_options(opt)\n",
        "\n",
        "        # set gpu ids\n",
        "        str_ids = opt.gpu_ids.split(',')\n",
        "        opt.gpu_ids = []\n",
        "        for str_id in str_ids:\n",
        "            id = int(str_id)\n",
        "            if id >= 0:\n",
        "                opt.gpu_ids.append(id)\n",
        "        #if len(opt.gpu_ids) > 0:\n",
        "            #torch.cuda.set_device(opt.gpu_ids[0])\n",
        "\n",
        "        assert len(opt.gpu_ids) == 0 or opt.batchSize % len(opt.gpu_ids) == 0, \\\n",
        "            \"Batch size %d is wrong. It must be a multiple of # GPUs %d.\" \\\n",
        "            % (opt.batchSize, len(opt.gpu_ids))\n",
        "\n",
        "        self.opt = opt\n",
        "        return self.opt"
      ],
      "metadata": {
        "id": "9NijQbcpb9y4"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TestOptions(BaseOptions):\n",
        "    def initialize(self, parser):\n",
        "        BaseOptions.initialize(self, parser)\n",
        "        parser.add_argument('--results_dir', type=str, default='./results/', help='saves results here.')\n",
        "        parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n",
        "        parser.add_argument('--how_many', type=int, default=float(\"inf\"), help='how many test images to run')\n",
        "        parser.add_argument('--show_input', action='store_true', help='show input images with the synthesized image')\n",
        "\n",
        "        parser.set_defaults(preprocess_mode='scale_width_and_crop', crop_size=256, load_size=256, display_winsize=256)\n",
        "        parser.set_defaults(serial_batches=True)\n",
        "        parser.set_defaults(no_flip=True)\n",
        "        parser.set_defaults(phase='test')\n",
        "        self.isTrain = False\n",
        "        return parser"
      ],
      "metadata": {
        "id": "hRHsKFqXb_Wc"
      },
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def id2label(id):\n",
        "    if id == 182:\n",
        "        id = 0\n",
        "    else:\n",
        "        id = id + 1\n",
        "    labelmap = \\\n",
        "        {0: 'unlabeled',\n",
        "         1: 'person',\n",
        "         2: 'bicycle',\n",
        "         3: 'car',\n",
        "         4: 'motorcycle',\n",
        "         5: 'airplane',\n",
        "         6: 'bus',\n",
        "         7: 'train',\n",
        "         8: 'truck',\n",
        "         9: 'boat',\n",
        "         10: 'traffic light',\n",
        "         11: 'fire hydrant',\n",
        "         12: 'street sign',\n",
        "         13: 'stop sign',\n",
        "         14: 'parking meter',\n",
        "         15: 'bench',\n",
        "         16: 'bird',\n",
        "         17: 'cat',\n",
        "         18: 'dog',\n",
        "         19: 'horse',\n",
        "         20: 'sheep',\n",
        "         21: 'cow',\n",
        "         22: 'elephant',\n",
        "         23: 'bear',\n",
        "         24: 'zebra',\n",
        "         25: 'giraffe',\n",
        "         26: 'hat',\n",
        "         27: 'backpack',\n",
        "         28: 'umbrella',\n",
        "         29: 'shoe',\n",
        "         30: 'eye glasses',\n",
        "         31: 'handbag',\n",
        "         32: 'tie',\n",
        "         33: 'suitcase',\n",
        "         34: 'frisbee',\n",
        "         35: 'skis',\n",
        "         36: 'snowboard',\n",
        "         37: 'sports ball',\n",
        "         38: 'kite',\n",
        "         39: 'baseball bat',\n",
        "         40: 'baseball glove',\n",
        "         41: 'skateboard',\n",
        "         42: 'surfboard',\n",
        "         43: 'tennis racket',\n",
        "         44: 'bottle',\n",
        "         45: 'plate',\n",
        "         46: 'wine glass',\n",
        "         47: 'cup',\n",
        "         48: 'fork',\n",
        "         49: 'knife',\n",
        "         50: 'spoon',\n",
        "         51: 'bowl',\n",
        "         52: 'banana',\n",
        "         53: 'apple',\n",
        "         54: 'sandwich',\n",
        "         55: 'orange',\n",
        "         56: 'broccoli',\n",
        "         57: 'carrot',\n",
        "         58: 'hot dog',\n",
        "         59: 'pizza',\n",
        "         60: 'donut',\n",
        "         61: 'cake',\n",
        "         62: 'chair',\n",
        "         63: 'couch',\n",
        "         64: 'potted plant',\n",
        "         65: 'bed',\n",
        "         66: 'mirror',\n",
        "         67: 'dining table',\n",
        "         68: 'window',\n",
        "         69: 'desk',\n",
        "         70: 'toilet',\n",
        "         71: 'door',\n",
        "         72: 'tv',\n",
        "         73: 'laptop',\n",
        "         74: 'mouse',\n",
        "         75: 'remote',\n",
        "         76: 'keyboard',\n",
        "         77: 'cell phone',\n",
        "         78: 'microwave',\n",
        "         79: 'oven',\n",
        "         80: 'toaster',\n",
        "         81: 'sink',\n",
        "         82: 'refrigerator',\n",
        "         83: 'blender',\n",
        "         84: 'book',\n",
        "         85: 'clock',\n",
        "         86: 'vase',\n",
        "         87: 'scissors',\n",
        "         88: 'teddy bear',\n",
        "         89: 'hair drier',\n",
        "         90: 'toothbrush',\n",
        "         91: 'hair brush',  # Last class of Thing\n",
        "         92: 'banner',  # Beginning of Stuff\n",
        "         93: 'blanket',\n",
        "         94: 'branch',\n",
        "         95: 'bridge',\n",
        "         96: 'building-other',\n",
        "         97: 'bush',\n",
        "         98: 'cabinet',\n",
        "         99: 'cage',\n",
        "         100: 'cardboard',\n",
        "         101: 'carpet',\n",
        "         102: 'ceiling-other',\n",
        "         103: 'ceiling-tile',\n",
        "         104: 'cloth',\n",
        "         105: 'clothes',\n",
        "         106: 'clouds',\n",
        "         107: 'counter',\n",
        "         108: 'cupboard',\n",
        "         109: 'curtain',\n",
        "         110: 'desk-stuff',\n",
        "         111: 'dirt',\n",
        "         112: 'door-stuff',\n",
        "         113: 'fence',\n",
        "         114: 'floor-marble',\n",
        "         115: 'floor-other',\n",
        "         116: 'floor-stone',\n",
        "         117: 'floor-tile',\n",
        "         118: 'floor-wood',\n",
        "         119: 'flower',\n",
        "         120: 'fog',\n",
        "         121: 'food-other',\n",
        "         122: 'fruit',\n",
        "         123: 'furniture-other',\n",
        "         124: 'grass',\n",
        "         125: 'gravel',\n",
        "         126: 'ground-other',\n",
        "         127: 'hill',\n",
        "         128: 'house',\n",
        "         129: 'leaves',\n",
        "         130: 'light',\n",
        "         131: 'mat',\n",
        "         132: 'metal',\n",
        "         133: 'mirror-stuff',\n",
        "         134: 'moss',\n",
        "         135: 'mountain',\n",
        "         136: 'mud',\n",
        "         137: 'napkin',\n",
        "         138: 'net',\n",
        "         139: 'paper',\n",
        "         140: 'pavement',\n",
        "         141: 'pillow',\n",
        "         142: 'plant-other',\n",
        "         143: 'plastic',\n",
        "         144: 'platform',\n",
        "         145: 'playingfield',\n",
        "         146: 'railing',\n",
        "         147: 'railroad',\n",
        "         148: 'river',\n",
        "         149: 'road',\n",
        "         150: 'rock',\n",
        "         151: 'roof',\n",
        "         152: 'rug',\n",
        "         153: 'salad',\n",
        "         154: 'sand',\n",
        "         155: 'sea',\n",
        "         156: 'shelf',\n",
        "         157: 'sky-other',\n",
        "         158: 'skyscraper',\n",
        "         159: 'snow',\n",
        "         160: 'solid-other',\n",
        "         161: 'stairs',\n",
        "         162: 'stone',\n",
        "         163: 'straw',\n",
        "         164: 'structural-other',\n",
        "         165: 'table',\n",
        "         166: 'tent',\n",
        "         167: 'textile-other',\n",
        "         168: 'towel',\n",
        "         169: 'tree',\n",
        "         170: 'vegetable',\n",
        "         171: 'wall-brick',\n",
        "         172: 'wall-concrete',\n",
        "         173: 'wall-other',\n",
        "         174: 'wall-panel',\n",
        "         175: 'wall-stone',\n",
        "         176: 'wall-tile',\n",
        "         177: 'wall-wood',\n",
        "         178: 'water-other',\n",
        "         179: 'waterdrops',\n",
        "         180: 'window-blind',\n",
        "         181: 'window-other',\n",
        "         182: 'wood'}\n",
        "    if id in labelmap:\n",
        "        return labelmap[id]\n",
        "    else:\n",
        "        return 'unknown'"
      ],
      "metadata": {
        "id": "2-qUVvQAcAof"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dominate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CC5_IrfverPn",
        "outputId": "f8e37993-6139-4757-c375-1f9de7fd9e73"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dominate in /usr/local/lib/python3.10/dist-packages (2.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import dominate\n",
        "from dominate.tags import *\n",
        "import os\n",
        "\n",
        "\n",
        "class HTML:\n",
        "    def __init__(self, web_dir, title, refresh=0):\n",
        "        if web_dir.endswith('.html'):\n",
        "            web_dir, html_name = os.path.split(web_dir)\n",
        "        else:\n",
        "            web_dir, html_name = web_dir, 'index.html'\n",
        "        self.title = title\n",
        "        self.web_dir = web_dir\n",
        "        self.html_name = html_name\n",
        "        self.img_dir = os.path.join(self.web_dir, 'images')\n",
        "        if len(self.web_dir) > 0 and not os.path.exists(self.web_dir):\n",
        "            os.makedirs(self.web_dir)\n",
        "        if len(self.web_dir) > 0 and not os.path.exists(self.img_dir):\n",
        "            os.makedirs(self.img_dir)\n",
        "\n",
        "        self.doc = dominate.document(title=title)\n",
        "        with self.doc:\n",
        "            h1(datetime.datetime.now().strftime(\"%I:%M%p on %B %d, %Y\"))\n",
        "        if refresh > 0:\n",
        "            with self.doc.head:\n",
        "                meta(http_equiv=\"refresh\", content=str(refresh))\n",
        "\n",
        "    def get_image_dir(self):\n",
        "        return self.img_dir\n",
        "\n",
        "    def add_header(self, str):\n",
        "        with self.doc:\n",
        "            h3(str)\n",
        "\n",
        "    def add_table(self, border=1):\n",
        "        self.t = table(border=border, style=\"table-layout: fixed;\")\n",
        "        self.doc.add(self.t)\n",
        "\n",
        "    def add_images(self, ims, txts, links, width=512):\n",
        "        self.add_table()\n",
        "        with self.t:\n",
        "            with tr():\n",
        "                for im, txt, link in zip(ims, txts, links):\n",
        "                    with td(style=\"word-wrap: break-word;\", halign=\"center\", valign=\"top\"):\n",
        "                        with p():\n",
        "                            with a(href=os.path.join('images', link)):\n",
        "                                img(style=\"width:%dpx\" % (width), src=os.path.join('images', im))\n",
        "                            br()\n",
        "                            p(txt.encode('utf-8'))\n",
        "\n",
        "    def save(self):\n",
        "        html_file = os.path.join(self.web_dir, self.html_name)\n",
        "        f = open(html_file, 'wt')\n",
        "        f.write(self.doc.render())\n",
        "        f.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    html = HTML('web/', 'test_html')\n",
        "    html.add_header('hello world')\n",
        "\n",
        "    ims = []\n",
        "    txts = []\n",
        "    links = []\n",
        "    for n in range(4):\n",
        "        ims.append('image_%d.jpg' % n)\n",
        "        txts.append('text_%d' % n)\n",
        "        links.append('image_%d.jpg' % n)\n",
        "    html.add_images(ims, txts, links)\n",
        "    html.save()"
      ],
      "metadata": {
        "id": "7UX1F6q9cEhI"
      },
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dill"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH81y9gCe05W",
        "outputId": "2f7ab7a5-9c30-4dea-ffef-2e945ca104a5"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (0.3.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jSGv0YV2e8QF"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import importlib\n",
        "import torch\n",
        "from argparse import Namespace\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import argparse\n",
        "import dill as pickle\n",
        "\n",
        "# Save an object to a file using pickle\n",
        "def save_obj(obj, name):\n",
        "    with open(name, 'wb') as f:\n",
        "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# Load an object from a file using pickle\n",
        "def load_obj(name):\n",
        "    with open(name, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Returns a configuration for creating a generator\n",
        "# |default_opt| should be the opt of the current experiment\n",
        "# |**kwargs|: if any configuration should be overridden, it can be specified here\n",
        "def copyconf(default_opt, **kwargs):\n",
        "    conf = vars(default_opt)\n",
        "    conf.update(kwargs)\n",
        "    return Namespace(**conf)\n",
        "\n",
        "# Tile a 3D numpy array of images for visualization\n",
        "def tile_images(imgs, picturesPerRow=4):\n",
        "    # Padding\n",
        "    padding = ((0, 0), (0, picturesPerRow - imgs.shape[0] % picturesPerRow), (0, 0))\n",
        "    imgs = np.pad(imgs, padding, mode='constant')\n",
        "\n",
        "    # Tiling Loop\n",
        "    tiled = [np.concatenate(imgs[i:i + picturesPerRow], axis=1) for i in range(0, imgs.shape[0], picturesPerRow)]\n",
        "    return np.concatenate(tiled, axis=0)\n",
        "\n",
        "# Converts a PyTorch Tensor into a NumPy array\n",
        "# |imtype|: the desired type of the converted numpy array\n",
        "def tensor2im(image_tensor, imtype=np.uint8, normalize=True, tile=False):\n",
        "    if isinstance(image_tensor, list):\n",
        "        image_numpy = []\n",
        "        for i in range(len(image_tensor)):\n",
        "            image_numpy.append(tensor2im(image_tensor[i], imtype, normalize))\n",
        "        return image_numpy\n",
        "\n",
        "    if image_tensor.dim() == 4:\n",
        "        # transform each image in the batch\n",
        "        images_np = []\n",
        "        for b in range(image_tensor.size(0)):\n",
        "            one_image = image_tensor[b]\n",
        "            one_image_np = tensor2im(one_image)\n",
        "            images_np.append(one_image_np.reshape(1, *one_image_np.shape))\n",
        "        images_np = np.concatenate(images_np, axis=0)\n",
        "        if tile:\n",
        "            images_tiled = tile_images(images_np)\n",
        "            return images_tiled\n",
        "        else:\n",
        "            return images_np\n",
        "\n",
        "    if image_tensor.dim() == 2:\n",
        "        image_tensor = image_tensor.unsqueeze(0)\n",
        "    image_numpy = image_tensor.detach().cpu().float().numpy()\n",
        "    if normalize:\n",
        "        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
        "    else:\n",
        "        image_numpy = np.transpose(image_numpy, (1, 2, 0)) * 255.0\n",
        "    image_numpy = np.clip(image_numpy, 0, 255)\n",
        "    if image_numpy.shape[2] == 1:\n",
        "        image_numpy = image_numpy[:, :, 0]\n",
        "    return image_numpy.astype(imtype)\n",
        "\n",
        "\n",
        "# Converts a one-hot tensor into a colorful label map\n",
        "def tensor2label(label_tensor, n_label, imtype=np.uint8, tile=False):\n",
        "    if label_tensor.dim() == 4:\n",
        "        # transform each image in the batch\n",
        "        images_np = []\n",
        "        for b in range(label_tensor.size(0)):\n",
        "            one_image = label_tensor[b]\n",
        "            one_image_np = tensor2label(one_image, n_label, imtype)\n",
        "            images_np.append(one_image_np.reshape(1, *one_image_np.shape))\n",
        "        images_np = np.concatenate(images_np, axis=0)\n",
        "        if tile:\n",
        "            images_tiled = tile_images(images_np)\n",
        "            return images_tiled\n",
        "        else:\n",
        "            images_np = images_np[0]\n",
        "            return images_np\n",
        "\n",
        "    if label_tensor.dim() == 1:\n",
        "        return np.zeros((64, 64, 3), dtype=np.uint8)\n",
        "    if n_label == 0:\n",
        "        return tensor2im(label_tensor, imtype)\n",
        "    label_tensor = label_tensor.cpu().float()\n",
        "    if label_tensor.size()[0] > 1:\n",
        "        label_tensor = label_tensor.max(0, keepdim=True)[1]\n",
        "    label_tensor = Colorize(n_label)(label_tensor)\n",
        "    label_numpy = np.transpose(label_tensor.numpy(), (1, 2, 0))\n",
        "    result = label_numpy.astype(imtype)\n",
        "    return result\n",
        "\n",
        "# Save a NumPy array as an image file\n",
        "def save_image(image_numpy, image_path, create_dir=False):\n",
        "    if create_dir:\n",
        "        os.makedirs(os.path.dirname(image_path), exist_ok=True)\n",
        "    if len(image_numpy.shape) == 2:\n",
        "        image_numpy = np.expand_dims(image_numpy, axis=2)\n",
        "    if image_numpy.shape[2] == 1:\n",
        "        image_numpy = np.repeat(image_numpy, 3, 2)\n",
        "    image_pil = Image.fromarray(image_numpy)\n",
        "\n",
        "    # Save to png\n",
        "    image_pil.save(os.path.join(image_path.replace('.jpg', '.png')))\n",
        "\n",
        "# Create directories if they do not exist\n",
        "def mkdirs(paths):\n",
        "    if isinstance(paths, list) and not isinstance(paths, str):\n",
        "        for path in paths:\n",
        "            os.makedirs(path, exist_ok=True)\n",
        "    else:\n",
        "        os.makedirs(paths, exist_ok=True)\n",
        "\n",
        "# Create a directory if it does not exist\n",
        "def mkdir(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "# Convert a string to an integer or leave it as a string\n",
        "def atoi(text):\n",
        "    return int(text) if text.isdigit() else text\n",
        "\n",
        "# Define a natural sorting order for strings\n",
        "def natural_keys(text):\n",
        "    '''\n",
        "    alist.sort(key=natural_keys) sorts in human order\n",
        "    http://nedbatchelder.com/blog/200712/human_sorting.html\n",
        "    (See Toothy's implementation in the comments)\n",
        "    '''\n",
        "    return [atoi(c) for c in re.split('(\\d+)', text)]\n",
        "\n",
        "# Sort a list of strings in natural order\n",
        "def natural_sort(items):\n",
        "    items.sort(key=natural_keys)\n",
        "\n",
        "# Convert a string to a boolean value\n",
        "def str2bool(v):\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
        "\n",
        "# Find a class in a module based on its name\n",
        "def find_class_in_module(target_cls_name, module):\n",
        "    target_cls_name = target_cls_name.replace('_', '').lower()\n",
        "    clslib = importlib.import_module(module)\n",
        "    cls = None\n",
        "    for name, clsobj in clslib.__dict__.items():\n",
        "        if name.lower() == target_cls_name:\n",
        "            cls = clsobj\n",
        "\n",
        "    if cls is None:\n",
        "        print(\"In %s, there should be a class whose name matches %s in lowercase without underscore(_)\" % (module, target_cls_name))\n",
        "        exit(0)\n",
        "\n",
        "    return cls\n",
        "\n",
        "# Save the weights of a PyTorch network\n",
        "def save_network(net, label, epoch, opt):\n",
        "    save_filename = '%s_net_%s.pth' % (epoch, label)\n",
        "    save_path = os.path.join(opt.checkpoints_dir, opt.name, save_filename)\n",
        "    torch.save(net.cpu().state_dict(), save_path)\n",
        "    if len(opt.gpu_ids) and torch.cuda.is_available():\n",
        "        net.cuda()\n",
        "\n",
        "# Load the weights into a PyTorch network\n",
        "def load_network(net, label, epoch, opt):\n",
        "    save_filename = '%s_net_%s.pth' % (epoch, label)\n",
        "    save_dir = os.path.join(opt.checkpoints_dir, opt.name)\n",
        "    save_path = os.path.join(save_dir, save_filename)\n",
        "    weights = torch.load(save_path)\n",
        "    net.load_state_dict(weights)\n",
        "    return net\n",
        "\n",
        "# Convert an integer to its binary representation\n",
        "def uint82bin(n, count=8):\n",
        "    \"\"\"returns the binary of integer n, count refers to amount of bits\"\"\"\n",
        "    return ''.join([str((n >> y) & 1) for y in range(count - 1, -1, -1)])\n",
        "\n",
        "# Create a color map for label visualization\n",
        "def labelcolormap(N):\n",
        "    if N == 35:  # cityscape\n",
        "        cmap = np.array([(0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (111, 74, 0), (81, 0, 81),\n",
        "                         (128, 64, 128), (244, 35, 232), (250, 170, 160), (230, 150, 140), (70, 70, 70), (102, 102, 156), (190, 153, 153),\n",
        "                         (180, 165, 180), (150, 100, 100), (150, 120, 90), (153, 153, 153), (153, 153, 153), (250, 170, 30), (220, 220, 0),\n",
        "                         (107, 142, 35), (152, 251, 152), (70, 130, 180), (220, 20, 60), (255, 0, 0), (0, 0, 142), (0, 0, 70),\n",
        "                         (0, 60, 100), (0, 0, 90), (0, 0, 110), (0, 80, 100), (0, 0, 230), (119, 11, 32), (0, 0, 142)],\n",
        "                        dtype=np.uint8)\n",
        "    else:\n",
        "        cmap = np.zeros((N, 3), dtype=np.uint8)\n",
        "        for i in range(N):\n",
        "            r, g, b = 0, 0, 0\n",
        "            id = i + 1  # let's give 0 a color\n",
        "            for j in range(7):\n",
        "                str_id = uint82bin(id)\n",
        "                r = r ^ (np.uint8(str_id[-1]) << (7 - j))\n",
        "                g = g ^ (np.uint8(str_id[-2]) << (7 - j))\n",
        "                b = b ^ (np.uint8(str_id[-3]) << (7 - j))\n",
        "                id = id >> 3\n",
        "            cmap[i, 0] = r\n",
        "            cmap[i, 1] = g\n",
        "            cmap[i, 2] = b\n",
        "\n",
        "        if N == 182:  # COCO\n",
        "            important_colors = {\n",
        "                'sea': (54, 62, 167),\n",
        "                'sky-other': (95, 219, 255),\n",
        "                'tree': (140, 104, 47),\n",
        "                'clouds': (170, 170, 170),\n",
        "                'grass': (29, 195, 49)\n",
        "            }\n",
        "            for i in range(N):\n",
        "                name = coco.id2label(i)\n",
        "                if name in important_colors:\n",
        "                    color = important_colors[name]\n",
        "                    cmap[i] = np.array(list(color))\n",
        "\n",
        "    return cmap\n",
        "\n",
        "# Colorize a grayscale image based on a predefined color map\n",
        "class Colorize(object):\n",
        "    def __init__(self, n=35):\n",
        "        self.cmap = labelcolormap(n)\n",
        "        self.cmap = torch.from_numpy(self.cmap[:n])\n",
        "\n",
        "    def __call__(self, gray_image):\n",
        "        size = gray_image.size()\n",
        "        color_image = torch.ByteTensor(3, size[1], size[2]).fill_(0)\n",
        "\n",
        "        for label in range(0, len(self.cmap)):\n",
        "            mask = (label == gray_image[0]).cpu()\n",
        "            color_image[0][mask] = self.cmap[label][0]\n",
        "            color_image[1][mask] = self.cmap[label][1]\n",
        "            color_image[2][mask] = self.cmap[label][2]\n",
        "\n",
        "        return color_image"
      ],
      "metadata": {
        "id": "NFM1r0UgcFsh"
      },
      "execution_count": 213,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import ntpath\n",
        "import time\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import scipy.misc\n",
        "from io import StringIO\n",
        "from io import BytesIO\n",
        "\n",
        "class Visualizer():\n",
        "    def __init__(self, opt):\n",
        "        self.opt = opt\n",
        "        self.tf_log = opt.isTrain and opt.tf_log\n",
        "        self.use_html = opt.isTrain and not opt.no_html\n",
        "        self.win_size = opt.display_winsize\n",
        "        self.name = opt.name\n",
        "        if self.tf_log:\n",
        "            self.tf = tf\n",
        "            self.log_dir = os.path.join(opt.checkpoints_dir, opt.name, 'logs')\n",
        "            self.writer = tf.summary.FileWriter(self.log_dir)\n",
        "\n",
        "        if self.use_html:\n",
        "            self.web_dir = os.path.join(opt.checkpoints_dir, opt.name, 'web')\n",
        "            self.img_dir = os.path.join(self.web_dir, 'images')\n",
        "            print('create web directory %s...' % self.web_dir)\n",
        "            mkdirs([self.web_dir, self.img_dir])\n",
        "        if opt.isTrain:\n",
        "            self.log_name = os.path.join(opt.checkpoints_dir, opt.name, 'loss_log.txt')\n",
        "            with open(self.log_name, \"a\") as log_file:\n",
        "                now = time.strftime(\"%c\")\n",
        "                log_file.write('================ Training Loss (%s) ================\\n' % now)\n",
        "\n",
        "    # |visuals|: dictionary of images to display or save\n",
        "    def display_current_results(self, visuals, epoch, step):\n",
        "\n",
        "        ## convert tensors to numpy arrays\n",
        "        visuals = self.convert_visuals_to_numpy(visuals)\n",
        "\n",
        "        if self.tf_log: # show images in tensorboard output\n",
        "            img_summaries = []\n",
        "            for label, image_numpy in visuals.items():\n",
        "                # Write the image to a string\n",
        "                try:\n",
        "                    s = StringIO()\n",
        "                except:\n",
        "                    s = BytesIO()\n",
        "                if len(image_numpy.shape) >= 4:\n",
        "                    image_numpy = image_numpy[0]\n",
        "                scipy.misc.toimage(image_numpy).save(s, format=\"jpeg\")\n",
        "                # Create an Image object\n",
        "                img_sum = self.tf.Summary.Image(encoded_image_string=s.getvalue(), height=image_numpy.shape[0], width=image_numpy.shape[1])\n",
        "                # Create a Summary value\n",
        "                img_summaries.append(self.tf.Summary.Value(tag=label, image=img_sum))\n",
        "\n",
        "            # Create and write Summary\n",
        "            summary = self.tf.Summary(value=img_summaries)\n",
        "            self.writer.add_summary(summary, step)\n",
        "\n",
        "        if self.use_html: # save images to a html file\n",
        "            img_path = os.path.join(self.img_dir, 'epoch%.3d_iter%.7d.png' % (epoch, step))\n",
        "            visuals_lst = []\n",
        "            for label, image_numpy in visuals.items():\n",
        "                if len(image_numpy.shape) >= 4:\n",
        "                    image_numpy = image_numpy[0]\n",
        "                visuals_lst.append(image_numpy)\n",
        "            image_cath = np.concatenate(visuals_lst, axis=0)\n",
        "            save_image(image_cath, img_path)\n",
        "\n",
        "            # update website\n",
        "            webpage = html.HTML(self.web_dir, 'Experiment name = %s' % self.name, refresh=5)\n",
        "            for n in range(epoch, 0, -1):\n",
        "                webpage.add_header('epoch [%d]' % n)\n",
        "                ims = []\n",
        "                txts = []\n",
        "                links = []\n",
        "\n",
        "                for label, image_numpy in visuals.items():\n",
        "                    if isinstance(image_numpy, list):\n",
        "                        for i in range(len(image_numpy)):\n",
        "                            img_path = 'epoch%.3d_iter%.3d_%s_%d.png' % (n, step, label, i)\n",
        "                            ims.append(img_path)\n",
        "                            txts.append(label+str(i))\n",
        "                            links.append(img_path)\n",
        "                    else:\n",
        "                        img_path = 'epoch%.3d_iter%.3d_%s.png' % (n, step, label)\n",
        "                        ims.append(img_path)\n",
        "                        txts.append(label)\n",
        "                        links.append(img_path)\n",
        "                if len(ims) < 10:\n",
        "                    webpage.add_images(ims, txts, links, width=self.win_size)\n",
        "                else:\n",
        "                    num = int(round(len(ims)/2.0))\n",
        "                    webpage.add_images(ims[:num], txts[:num], links[:num], width=self.win_size)\n",
        "                    webpage.add_images(ims[num:], txts[num:], links[num:], width=self.win_size)\n",
        "            webpage.save()\n",
        "\n",
        "    # errors: dictionary of error labels and values\n",
        "    def plot_current_errors(self, errors, step):\n",
        "        if self.tf_log:\n",
        "            for tag, value in errors.items():\n",
        "                value = value.mean().float()\n",
        "                summary = self.tf.Summary(value=[self.tf.Summary.Value(tag=tag, simple_value=value)])\n",
        "                self.writer.add_summary(summary, step)\n",
        "\n",
        "    # errors: same format as |errors| of plotCurrentErrors\n",
        "    def print_current_errors(self, epoch, i, errors, t):\n",
        "        message = '(epoch: %d, iters: %d, time: %.3f) ' % (epoch, i, t)\n",
        "        for k, v in errors.items():\n",
        "            #print(v)\n",
        "            #if v != 0:\n",
        "            v = v.mean().float()\n",
        "            message += '%s: %.3f ' % (k, v)\n",
        "\n",
        "        print(message)\n",
        "        with open(self.log_name, \"a\") as log_file:\n",
        "            log_file.write('%s\\n' % message)\n",
        "\n",
        "    def convert_visuals_to_numpy(self, visuals):\n",
        "        for key, t in visuals.items():\n",
        "            tile = self.opt.batchSize > 8\n",
        "            if 'input_label' == key:\n",
        "                t = tensor2label(t, self.opt.label_nc, tile=tile)\n",
        "            else:\n",
        "                t = tensor2im(t, tile=tile)\n",
        "            visuals[key] = t\n",
        "        return visuals\n",
        "\n",
        "    # save image to the disk\n",
        "    def save_images(self, webpage, visuals, image_path):\n",
        "        visuals = self.convert_visuals_to_numpy(visuals)\n",
        "\n",
        "        image_dir = webpage.get_image_dir()\n",
        "        short_path = ntpath.basename(image_path[0])\n",
        "        name = os.path.splitext(short_path)[0]\n",
        "\n",
        "        visuals_lst = []\n",
        "        image_name = '%s.png' % name\n",
        "        save_path = os.path.join(image_dir, image_name)\n",
        "        for label, image_numpy in visuals.items():\n",
        "            visuals_lst.append(image_numpy)\n",
        "\n",
        "        image_cath = np.concatenate(visuals_lst, axis=1)\n",
        "        save_image(image_cath, save_path, create_dir=True)"
      ],
      "metadata": {
        "id": "j221Wxl5cG4N"
      },
      "execution_count": 214,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import argparse\n",
        "import os\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "\n",
        "class BaseOptions():\n",
        "    def __init__(self):\n",
        "        self.initialized = False\n",
        "\n",
        "    def initialize(self, parser):\n",
        "        # experiment specifics\n",
        "        parser.add_argument('--name', type=str, default='ast_summer2winteryosemite', help='name of the experiment. It decides where to store samples and models')\n",
        "        parser.add_argument('--task', type=str, default='AST', help='task type: AST | SIS | MMIS')\n",
        "\n",
        "        parser.add_argument('--gpu_ids', type=str, default='0', help='gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU')\n",
        "        parser.add_argument('--checkpoints_dir', type=str, default='./checkpoints', help='models are saved here')\n",
        "        parser.add_argument('--model', type=str, default='pix2pix', help='which model to use')\n",
        "        parser.add_argument('--norm_G', type=str, default='spectralinstance', help='instance normalization or batch normalization')\n",
        "        parser.add_argument('--norm_D', type=str, default='spectralinstance', help='instance normalization or batch normalization')\n",
        "        parser.add_argument('--norm_S', type=str, default='spectralinstance', help='instance normalization or batch normalization')\n",
        "        parser.add_argument('--norm_E', type=str, default='spectralinstance', help='instance normalization or batch normalization')\n",
        "        parser.add_argument('--phase', type=str, default='train', help='train, val, test, etc')\n",
        "\n",
        "        # input/output sizes\n",
        "        parser.add_argument('--batchSize', type=int, default=1, help='input batch size')\n",
        "        parser.add_argument('--preprocess_mode', type=str, default='scale_width_and_crop', help='scaling and cropping of images at load time.', choices=(\"resize_and_crop\", \"crop\", \"scale_width\", \"scale_width_and_crop\", \"scale_shortside\", \"scale_shortside_and_crop\", \"fixed\", \"none\"))\n",
        "        parser.add_argument('--load_size', type=int, default=1024, help='Scale images to this size. The final image will be cropped to --crop_size.')\n",
        "        parser.add_argument('--crop_size', type=int, default=512, help='Crop to the width of crop_size (after initially scaling the images to load_size.)')\n",
        "        parser.add_argument('--aspect_ratio', type=float, default=1.0, help='The ratio width/height. The final height of the load image will be crop_size/aspect_ratio')\n",
        "        parser.add_argument('--label_nc', type=int, default=3, help='# of input label classes without unknown class. If you have unknown class as class label, specify --contain_dopntcare_label.')\n",
        "        parser.add_argument('--contain_dontcare_label', action='store_true', help='if the label map contains dontcare label (dontcare=255)')\n",
        "        parser.add_argument('--output_nc', type=int, default=3, help='# of output image channels')\n",
        "\n",
        "        # for setting inputs\n",
        "        parser.add_argument('--dataset_mode', type=str, default='summer2winteryosemite')\n",
        "        parser.add_argument('--croot', type=str, default='./datasets/summer2winter_yosemite/', help='content dataroot')\n",
        "        parser.add_argument('--sroot', type=str, default='./datasets/summer2winter_yosemite/', help='style dataroot')\n",
        "        parser.add_argument('--serial_batches', action='store_true', help='if true, takes images in order to make batches, otherwise takes them randomly')\n",
        "        parser.add_argument('--no_flip', action='store_true', help='if specified, do not flip the images for data argumentation')\n",
        "        parser.add_argument('--nThreads', default=0, type=int, help='# threads for loading data')\n",
        "        parser.add_argument('--max_dataset_size', type=int, default=sys.maxsize, help='Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.')\n",
        "        parser.add_argument('--load_from_opt_file', action='store_true', help='load the options from checkpoints and use that as default')\n",
        "        parser.add_argument('--cache_filelist_write', action='store_true', help='saves the current filelist into a text file, so that it loads faster')\n",
        "        parser.add_argument('--cache_filelist_read', action='store_true', help='reads from the file list cache')\n",
        "\n",
        "        # for displays\n",
        "        parser.add_argument('--display_winsize', type=int, default=400, help='display window size')\n",
        "\n",
        "        # for generator\n",
        "        parser.add_argument('--netG', type=str, default='tsit', help='selects model to use for netG (tsit | pix2pixhd)')\n",
        "        parser.add_argument('--ngf', type=int, default=64, help='# of gen filters in first conv layer')\n",
        "        parser.add_argument('--init_type', type=str, default='xavier', help='network initialization [normal|xavier|kaiming|orthogonal]')\n",
        "        parser.add_argument('--init_variance', type=float, default=0.02, help='variance of the initialization distribution')\n",
        "        parser.add_argument('--z_dim', type=int, default=256, help=\"dimension of the latent z vector\")\n",
        "        parser.add_argument('--alpha', type=float, default=1.0, help='The parameter that controls the degree of stylization (between 0 and 1)')\n",
        "        parser.add_argument('--no_ss', action='store_true', help='discard the style stream (better results in certain cases).')\n",
        "\n",
        "        # for instance-wise features\n",
        "        parser.add_argument('--no_instance', action='store_true', help='if specified, do *not* add instance map as input')\n",
        "        parser.add_argument('--nef', type=int, default=16, help='# of encoder filters in the first conv layer')\n",
        "        parser.add_argument('--use_vae', action='store_true', help='enable training with an image encoder.')\n",
        "\n",
        "        self.initialized = True\n",
        "\n",
        "        return parser\n",
        "\n",
        "    def gather_options(self):\n",
        "        # initialize parser with basic options\n",
        "        if not self.initialized:\n",
        "            parser = argparse.ArgumentParser(\n",
        "                formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "            parser = self.initialize(parser)\n",
        "\n",
        "        # get the basic options\n",
        "        opt, unknown = parser.parse_known_args()\n",
        "\n",
        "        # modify model-related parser options\n",
        "        model_name = opt.model\n",
        "        model_option_setter = get_option_setter(model_name)\n",
        "        parser = model_option_setter(parser, self.isTrain)\n",
        "\n",
        "        # modify dataset-related parser options\n",
        "        dataset_mode = opt.dataset_mode\n",
        "        dataset_option_setter = data.get_option_setter(dataset_mode)\n",
        "        parser = dataset_option_setter(parser, self.isTrain)\n",
        "\n",
        "        opt, unknown = parser.parse_known_args()\n",
        "\n",
        "        # if there is opt_file, load it.\n",
        "        # The previous default options will be overwritten\n",
        "        if opt.load_from_opt_file:\n",
        "            parser = self.update_options_from_file(parser, opt)\n",
        "\n",
        "        opt = parser.parse_args()\n",
        "        self.parser = parser\n",
        "        return opt\n",
        "\n",
        "    def print_options(self, opt):\n",
        "        message = ''\n",
        "        message += '----------------- Options ---------------\\n'\n",
        "        for k, v in sorted(vars(opt).items()):\n",
        "            comment = ''\n",
        "            default = self.parser.get_default(k)\n",
        "            if v != default:\n",
        "                comment = '\\t[default: %s]' % str(default)\n",
        "            message += '{:>25}: {:<30}{}\\n'.format(str(k), str(v), comment)\n",
        "        message += '----------------- End -------------------'\n",
        "        print(message)\n",
        "\n",
        "    def option_file_path(self, opt, makedir=False):\n",
        "        expr_dir = os.path.join(opt.checkpoints_dir, opt.name)\n",
        "        if makedir:\n",
        "            mkdirs(expr_dir)\n",
        "        file_name = os.path.join(expr_dir, 'opt')\n",
        "        return file_name\n",
        "\n",
        "    def save_options(self, opt):\n",
        "        file_name = self.option_file_path(opt, makedir=True)\n",
        "        with open(file_name + '.txt', 'wt') as opt_file:\n",
        "            for k, v in sorted(vars(opt).items()):\n",
        "                comment = ''\n",
        "                default = self.parser.get_default(k)\n",
        "                if v != default:\n",
        "                    comment = '\\t[default: %s]' % str(default)\n",
        "                opt_file.write('{:>25}: {:<30}{}\\n'.format(str(k), str(v), comment))\n",
        "\n",
        "        with open(file_name + '.pkl', 'wb') as opt_file:\n",
        "            pickle.dump(opt, opt_file)\n",
        "\n",
        "    def update_options_from_file(self, parser, opt):\n",
        "        new_opt = self.load_options(opt)\n",
        "        for k, v in sorted(vars(opt).items()):\n",
        "            if hasattr(new_opt, k) and v != getattr(new_opt, k):\n",
        "                new_val = getattr(new_opt, k)\n",
        "                parser.set_defaults(**{k: new_val})\n",
        "        return parser\n",
        "\n",
        "    def load_options(self, opt):\n",
        "        file_name = self.option_file_path(opt, makedir=False)\n",
        "        new_opt = pickle.load(open(file_name + '.pkl', 'rb'))\n",
        "        return new_opt\n",
        "\n",
        "    def parse(self, save=False):\n",
        "\n",
        "        opt = self.gather_options()\n",
        "        opt.num_upsampling_layers = 'more'\n",
        "        opt.isTrain = self.isTrain   # train or test\n",
        "        assert opt.task == 'AST' or opt.task == 'SIS' or opt.task == 'MMIS', \\\n",
        "            f'Task type should be: AST | SIS | MMIS, but got {opt.task}.'\n",
        "\n",
        "        # Set semantic_nc based on the option.\n",
        "        # This will be convenient in many places\n",
        "        if opt.task == 'SIS':\n",
        "            opt.semantic_nc = opt.label_nc + \\\n",
        "                              (1 if opt.contain_dontcare_label else 0) + \\\n",
        "                              (0 if opt.no_instance else 1)\n",
        "            opt.no_ss = True\n",
        "        else:\n",
        "            opt.semantic_nc = 3\n",
        "\n",
        "        self.print_options(opt)\n",
        "        if opt.isTrain:\n",
        "            self.save_options(opt)\n",
        "\n",
        "        # set gpu ids\n",
        "        str_ids = opt.gpu_ids.split(',')\n",
        "        opt.gpu_ids = []\n",
        "        for str_id in str_ids:\n",
        "            id = int(str_id)\n",
        "            if id >= 0:\n",
        "                opt.gpu_ids.append(id)\n",
        "        #if len(opt.gpu_ids) > 0:\n",
        "            #torch.cuda.set_device(opt.gpu_ids[0])\n",
        "\n",
        "        assert len(opt.gpu_ids) == 0 or opt.batchSize % len(opt.gpu_ids) == 0, \\\n",
        "            \"Batch size %d is wrong. It must be a multiple of # GPUs %d.\" \\\n",
        "            % (opt.batchSize, len(opt.gpu_ids))\n",
        "\n",
        "        self.opt = opt\n",
        "        return self.opt"
      ],
      "metadata": {
        "id": "pIOloDlifZ10"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class pix2pixmodel(torch.nn.Module):\n",
        "    @staticmethod\n",
        "    def modify_commandline_options(parser, is_train):\n",
        "        modify_commandline_options(parser, is_train)\n",
        "        return parser\n",
        "\n",
        "    def __init__(self, opt):\n",
        "        super().__init__()\n",
        "        self.opt = opt\n",
        "        self.FloatTensor = torch.cuda.FloatTensor if self.use_gpu() \\\n",
        "            else torch.FloatTensor\n",
        "        self.ByteTensor = torch.cuda.ByteTensor if self.use_gpu() \\\n",
        "            else torch.ByteTensor\n",
        "\n",
        "        self.netG, self.netD, self.netE = self.initialize_networks(opt)\n",
        "\n",
        "        # set loss functions\n",
        "        if opt.isTrain:\n",
        "            self.criterionGAN = GANLoss(\n",
        "                opt.gan_mode, tensor=self.FloatTensor, opt=self.opt)\n",
        "            self.criterionFeat = torch.nn.L1Loss()\n",
        "            if not opt.no_vgg_loss:\n",
        "                self.criterionVGG = VGGLoss(self.opt.gpu_ids)\n",
        "            if opt.use_vae:\n",
        "                self.KLDLoss = KLDLoss()\n",
        "\n",
        "    # Entry point for all calls involving forward pass\n",
        "    # of deep networks. We used this approach since DataParallel module\n",
        "    # can't parallelize custom functions, we branch to different\n",
        "    # routines based on |mode|.\n",
        "    def forward(self, data, mode):\n",
        "        input_semantics, real_image = self.preprocess_input(data)\n",
        "        if mode == 'generator':\n",
        "            g_loss, generated = self.compute_generator_loss(\n",
        "                input_semantics, real_image)\n",
        "            return g_loss, generated\n",
        "        elif mode == 'discriminator':\n",
        "            d_loss = self.compute_discriminator_loss(\n",
        "                input_semantics, real_image)\n",
        "            return d_loss\n",
        "        elif mode == 'encode_only':\n",
        "            z, mu, logvar = self.encode_z(real_image)\n",
        "            return mu, logvar\n",
        "        elif mode == 'inference':\n",
        "            with torch.no_grad():\n",
        "                fake_image, _ = self.generate_fake(input_semantics, real_image)\n",
        "            return fake_image\n",
        "        else:\n",
        "            raise ValueError(\"|mode| is invalid\")\n",
        "\n",
        "    def create_optimizers(self, opt):\n",
        "        G_params = list(self.netG.parameters())\n",
        "        if opt.use_vae:\n",
        "            G_params += list(self.netE.parameters())\n",
        "        if opt.isTrain:\n",
        "            D_params = list(self.netD.parameters())\n",
        "\n",
        "        if opt.no_TTUR:\n",
        "            beta1, beta2 = opt.beta1, opt.beta2\n",
        "            G_lr, D_lr = opt.lr, opt.lr\n",
        "        else:\n",
        "            beta1, beta2 = 0, 0.9\n",
        "            G_lr, D_lr = opt.lr / 2, opt.lr * 2\n",
        "\n",
        "        optimizer_G = torch.optim.Adam(G_params, lr=G_lr, betas=(beta1, beta2))\n",
        "        optimizer_D = torch.optim.Adam(D_params, lr=D_lr, betas=(beta1, beta2))\n",
        "\n",
        "        return {'optimizer_G': optimizer_G, 'optimizer_D': optimizer_D}\n",
        "\n",
        "\n",
        "    def save(self, epoch):\n",
        "        save_network(self.netG, 'G', epoch, self.opt)\n",
        "        save_network(self.netD, 'D', epoch, self.opt)\n",
        "        if self.opt.use_vae:\n",
        "            save_network(self.netE, 'E', epoch, self.opt)\n",
        "\n",
        "    ############################################################################\n",
        "    # Private helper methods\n",
        "    ############################################################################\n",
        "\n",
        "    def initialize_networks(self, opt):\n",
        "        netG = define_G(opt)\n",
        "        netD = define_D(opt) if opt.isTrain else None\n",
        "        netE = define_E(opt) if opt.use_vae else None\n",
        "\n",
        "        if not opt.isTrain or opt.continue_train:\n",
        "            netG = load_network(netG, 'G', opt.which_epoch, opt)\n",
        "            if opt.isTrain:\n",
        "                netD = load_network(netD, 'D', opt.which_epoch, opt)\n",
        "            if opt.use_vae:\n",
        "                netE = load_network(netE, 'E', opt.which_epoch, opt)\n",
        "\n",
        "        return netG, netD, netE\n",
        "\n",
        "    # preprocess the input, such as moving the tensors to GPUs\n",
        "    # and transforming the label map to one-hot encoding (for SIS)\n",
        "    # |data|: dictionary of the input data\n",
        "    def preprocess_input(self, data):\n",
        "        # move to GPU and change data types\n",
        "        if self.opt.task == 'SIS':\n",
        "            data['label'] = data['label'].long()\n",
        "        if self.use_gpu():\n",
        "            data['label'] = data['label'].cuda()\n",
        "            data['instance'] = data['instance'].cuda()\n",
        "            data['image'] = data['image'].cuda()\n",
        "\n",
        "        # create one-hot label map for SIS\n",
        "        if self.opt.task == 'SIS':\n",
        "            label_map = data['label']\n",
        "            bs, _, h, w = label_map.size()\n",
        "            nc = self.opt.label_nc + 1 if self.opt.contain_dontcare_label \\\n",
        "                else self.opt.label_nc\n",
        "            input_label = self.FloatTensor(bs, nc, h, w).zero_()\n",
        "            input_semantics = input_label.scatter_(1, label_map, 1.0)\n",
        "\n",
        "            # concatenate instance map if it exists\n",
        "            if not self.opt.no_instance:\n",
        "                inst_map = data['instance']\n",
        "                instance_edge_map = self.get_edges(inst_map)\n",
        "                input_semantics = torch.cat((input_semantics, instance_edge_map), dim=1)\n",
        "        else:\n",
        "            input_semantics = data['label']\n",
        "\n",
        "        return input_semantics, data['image']\n",
        "\n",
        "    def compute_generator_loss(self, content, style):\n",
        "        G_losses = {}\n",
        "\n",
        "        fake_image, KLD_loss = self.generate_fake(\n",
        "            content, style, compute_kld_loss=self.opt.use_vae)\n",
        "\n",
        "        if self.opt.use_vae:\n",
        "            G_losses['KLD'] = KLD_loss\n",
        "\n",
        "        if self.opt.task == 'SIS':\n",
        "            pred_fake, pred_real = self.discriminate(fake_image, style, content)\n",
        "        else:\n",
        "            pred_fake, pred_real = self.discriminate(fake_image, style)\n",
        "\n",
        "        G_losses['GAN'] = self.criterionGAN(pred_fake, True,\n",
        "                                            for_discriminator=False)\n",
        "\n",
        "        if not self.opt.no_ganFeat_loss:\n",
        "            num_D = len(pred_fake)\n",
        "            GAN_Feat_loss = self.FloatTensor(1).fill_(0)\n",
        "            for i in range(num_D):  # for each discriminator\n",
        "                # last output is the final prediction, so we exclude it\n",
        "                num_intermediate_outputs = len(pred_fake[i]) - 1\n",
        "                for j in range(num_intermediate_outputs):  # for each layer output\n",
        "                    unweighted_loss = self.criterionFeat(\n",
        "                        pred_fake[i][j], pred_real[i][j].detach())\n",
        "                    GAN_Feat_loss += unweighted_loss * self.opt.lambda_feat / num_D\n",
        "            G_losses['GAN_Feat'] = GAN_Feat_loss\n",
        "\n",
        "        if not self.opt.no_vgg_loss:\n",
        "            target = style if self.opt.task == 'SIS' else content\n",
        "            G_losses['VGG'] = self.criterionVGG(fake_image, target) * self.opt.lambda_vgg\n",
        "\n",
        "        return G_losses, fake_image\n",
        "\n",
        "    def compute_discriminator_loss(self, content, style):\n",
        "        D_losses = {}\n",
        "        with torch.no_grad():\n",
        "            fake_image, _ = self.generate_fake(content, style)\n",
        "            fake_image = fake_image.detach()\n",
        "            fake_image.requires_grad_()\n",
        "\n",
        "        if self.opt.task == 'SIS':\n",
        "            pred_fake, pred_real = self.discriminate(fake_image, style, content)\n",
        "        else:\n",
        "            pred_fake, pred_real = self.discriminate(fake_image, style)\n",
        "\n",
        "        D_losses['D_Fake'] = self.criterionGAN(pred_fake, False,\n",
        "                                               for_discriminator=True)\n",
        "        D_losses['D_real'] = self.criterionGAN(pred_real, True,\n",
        "                                               for_discriminator=True)\n",
        "\n",
        "        return D_losses\n",
        "\n",
        "    def encode_z(self, real_image):\n",
        "        mu, logvar = self.netE(real_image)\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return z, mu, logvar\n",
        "\n",
        "    def generate_fake(self, input_semantics, real_image, compute_kld_loss=False):\n",
        "        z = None\n",
        "        KLD_loss = None\n",
        "        if self.opt.use_vae:\n",
        "            z, mu, logvar = self.encode_z(real_image)\n",
        "            if compute_kld_loss:\n",
        "                KLD_loss = self.KLDLoss(mu, logvar) * self.opt.lambda_kld\n",
        "\n",
        "        fake_image = self.netG(input_semantics, real_image, z=z)\n",
        "\n",
        "        assert (not compute_kld_loss) or self.opt.use_vae, \\\n",
        "            \"You cannot compute KLD loss if opt.use_vae == False\"\n",
        "\n",
        "        return fake_image, KLD_loss\n",
        "\n",
        "    # Given fake and real image, return the prediction of discriminator\n",
        "    # for each fake and real image. The condition is used in SIS.\n",
        "    def discriminate(self, fake_image, real_image, condition=None):\n",
        "        if self.opt.task == 'SIS':\n",
        "            assert condition is not None\n",
        "            fake_concat = torch.cat([condition, fake_image], dim=1)\n",
        "            real_concat = torch.cat([condition, real_image], dim=1)\n",
        "        else:\n",
        "            assert condition is None\n",
        "            fake_concat = fake_image\n",
        "            real_concat = real_image\n",
        "\n",
        "        # In Batch Normalization, the fake and real images are\n",
        "        # recommended to be in the same batch to avoid disparate\n",
        "        # statistics in fake and real images.\n",
        "        # So both fake and real images are fed to D all at once.\n",
        "        fake_and_real = torch.cat([fake_concat, real_concat], dim=0)\n",
        "\n",
        "        discriminator_out = self.netD(fake_and_real)\n",
        "\n",
        "        pred_fake, pred_real = self.divide_pred(discriminator_out)\n",
        "\n",
        "        return pred_fake, pred_real\n",
        "\n",
        "    # Take the prediction of fake and real images from the combined batch\n",
        "    def divide_pred(self, pred):\n",
        "        # the prediction contains the intermediate outputs of multi-scale GAN,\n",
        "        # so it's usually a list\n",
        "        if type(pred) == list:\n",
        "            fake = []\n",
        "            real = []\n",
        "            for p in pred:\n",
        "                fake.append([tensor[:tensor.size(0) // 2] for tensor in p])\n",
        "                real.append([tensor[tensor.size(0) // 2:] for tensor in p])\n",
        "        else:\n",
        "            fake = pred[:pred.size(0) // 2]\n",
        "            real = pred[pred.size(0) // 2:]\n",
        "\n",
        "        return fake, real\n",
        "\n",
        "    def get_edges(self, t):\n",
        "        edge = self.ByteTensor(t.size()).zero_()\n",
        "        edge[:, :, :, 1:] = edge[:, :, :, 1:] | (t[:, :, :, 1:] != t[:, :, :, :-1])\n",
        "        edge[:, :, :, :-1] = edge[:, :, :, :-1] | (t[:, :, :, 1:] != t[:, :, :, :-1])\n",
        "        edge[:, :, 1:, :] = edge[:, :, 1:, :] | (t[:, :, 1:, :] != t[:, :, :-1, :])\n",
        "        edge[:, :, :-1, :] = edge[:, :, :-1, :] | (t[:, :, 1:, :] != t[:, :, :-1, :])\n",
        "        return edge.float()\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return eps.mul(std) + mu\n",
        "\n",
        "    def use_gpu(self):\n",
        "        return len(self.opt.gpu_ids) > 0"
      ],
      "metadata": {
        "id": "z8sdDmtchCfG"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import dominate\n",
        "from dominate.tags import *\n",
        "import os\n",
        "\n",
        "\n",
        "class HTML:\n",
        "    def __init__(self, web_dir, title, refresh=0):\n",
        "        if web_dir.endswith('.html'):\n",
        "            web_dir, html_name = os.path.split(web_dir)\n",
        "        else:\n",
        "            web_dir, html_name = web_dir, 'index.html'\n",
        "        self.title = title\n",
        "        self.web_dir = web_dir\n",
        "        self.html_name = html_name\n",
        "        self.img_dir = os.path.join(self.web_dir, 'images')\n",
        "        if len(self.web_dir) > 0 and not os.path.exists(self.web_dir):\n",
        "            os.makedirs(self.web_dir)\n",
        "        if len(self.web_dir) > 0 and not os.path.exists(self.img_dir):\n",
        "            os.makedirs(self.img_dir)\n",
        "\n",
        "        self.doc = dominate.document(title=title)\n",
        "        with self.doc:\n",
        "            h1(datetime.datetime.now().strftime(\"%I:%M%p on %B %d, %Y\"))\n",
        "        if refresh > 0:\n",
        "            with self.doc.head:\n",
        "                meta(http_equiv=\"refresh\", content=str(refresh))\n",
        "\n",
        "    def get_image_dir(self):\n",
        "        return self.img_dir\n",
        "\n",
        "    def add_header(self, str):\n",
        "        with self.doc:\n",
        "            h3(str)\n",
        "\n",
        "    def add_table(self, border=1):\n",
        "        self.t = table(border=border, style=\"table-layout: fixed;\")\n",
        "        self.doc.add(self.t)\n",
        "\n",
        "    def add_images(self, ims, txts, links, width=512):\n",
        "        self.add_table()\n",
        "        with self.t:\n",
        "            with tr():\n",
        "                for im, txt, link in zip(ims, txts, links):\n",
        "                    with td(style=\"word-wrap: break-word;\", halign=\"center\", valign=\"top\"):\n",
        "                        with p():\n",
        "                            with a(href=os.path.join('images', link)):\n",
        "                                img(style=\"width:%dpx\" % (width), src=os.path.join('images', im))\n",
        "                            br()\n",
        "                            p(txt.encode('utf-8'))\n",
        "\n",
        "    def save(self):\n",
        "        html_file = os.path.join(self.web_dir, self.html_name)\n",
        "        f = open(html_file, 'wt')\n",
        "        f.write(self.doc.render())\n",
        "        f.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    html = HTML('web/', 'test_html')\n",
        "    html.add_header('hello world')\n",
        "\n",
        "    ims = []\n",
        "    txts = []\n",
        "    links = []\n",
        "    for n in range(4):\n",
        "        ims.append('image_%d.jpg' % n)\n",
        "        txts.append('text_%d' % n)\n",
        "        links.append('image_%d.jpg' % n)\n",
        "    html.add_images(ims, txts, links)\n",
        "    html.save()"
      ],
      "metadata": {
        "id": "dmF_zD77iML0"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TestOptions(BaseOptions):\n",
        "    def initialize(self, parser):\n",
        "        BaseOptions.initialize(self, parser)\n",
        "        parser.add_argument('--results_dir', type=str, default='./results/', help='saves results here.')\n",
        "        parser.add_argument('--which_epoch', type=str, default='latest', help='which epoch to load? set to latest to use latest cached model')\n",
        "        parser.add_argument('--how_many', type=int, default=float(\"inf\"), help='how many test images to run')\n",
        "        parser.add_argument('--show_input', action='store_true', help='show input images with the synthesized image')\n",
        "\n",
        "        parser.set_defaults(preprocess_mode='scale_width_and_crop', crop_size=256, load_size=256, display_winsize=256)\n",
        "        parser.set_defaults(serial_batches=True)\n",
        "        parser.set_defaults(no_flip=True)\n",
        "        parser.set_defaults(phase='test')\n",
        "        self.isTrain = False\n",
        "        return parser"
      ],
      "metadata": {
        "id": "QUNyKNwliSSg"
      },
      "execution_count": 218,
      "outputs": []
    }
  ]
}